{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>...</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cardiovasc_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1803.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iso_code continent     location        date  total_cases  new_cases  \\\n",
       "0       AFG      Asia  Afghanistan  2019-12-31          0.0        0.0   \n",
       "1       AFG      Asia  Afghanistan  2020-01-01          0.0        0.0   \n",
       "2       AFG      Asia  Afghanistan  2020-01-02          0.0        0.0   \n",
       "3       AFG      Asia  Afghanistan  2020-01-03          0.0        0.0   \n",
       "4       AFG      Asia  Afghanistan  2020-01-04          0.0        0.0   \n",
       "5       AFG      Asia  Afghanistan  2020-01-05          0.0        0.0   \n",
       "6       AFG      Asia  Afghanistan  2020-01-06          0.0        0.0   \n",
       "7       AFG      Asia  Afghanistan  2020-01-07          0.0        0.0   \n",
       "8       AFG      Asia  Afghanistan  2020-01-08          0.0        0.0   \n",
       "9       AFG      Asia  Afghanistan  2020-01-09          0.0        0.0   \n",
       "10      AFG      Asia  Afghanistan  2020-01-10          0.0        0.0   \n",
       "11      AFG      Asia  Afghanistan  2020-01-11          0.0        0.0   \n",
       "12      AFG      Asia  Afghanistan  2020-01-12          0.0        0.0   \n",
       "13      AFG      Asia  Afghanistan  2020-01-13          0.0        0.0   \n",
       "14      AFG      Asia  Afghanistan  2020-01-14          0.0        0.0   \n",
       "15      AFG      Asia  Afghanistan  2020-01-15          0.0        0.0   \n",
       "16      AFG      Asia  Afghanistan  2020-01-16          0.0        0.0   \n",
       "17      AFG      Asia  Afghanistan  2020-01-17          0.0        0.0   \n",
       "18      AFG      Asia  Afghanistan  2020-01-18          0.0        0.0   \n",
       "19      AFG      Asia  Afghanistan  2020-01-19          0.0        0.0   \n",
       "20      AFG      Asia  Afghanistan  2020-01-20          0.0        0.0   \n",
       "21      AFG      Asia  Afghanistan  2020-01-21          0.0        0.0   \n",
       "22      AFG      Asia  Afghanistan  2020-01-22          0.0        0.0   \n",
       "23      AFG      Asia  Afghanistan  2020-01-23          0.0        0.0   \n",
       "24      AFG      Asia  Afghanistan  2020-01-24          0.0        0.0   \n",
       "25      AFG      Asia  Afghanistan  2020-01-25          0.0        0.0   \n",
       "26      AFG      Asia  Afghanistan  2020-01-26          0.0        0.0   \n",
       "27      AFG      Asia  Afghanistan  2020-01-27          0.0        0.0   \n",
       "28      AFG      Asia  Afghanistan  2020-01-28          0.0        0.0   \n",
       "29      AFG      Asia  Afghanistan  2020-01-29          0.0        0.0   \n",
       "30      AFG      Asia  Afghanistan  2020-01-30          0.0        0.0   \n",
       "31      AFG      Asia  Afghanistan  2020-01-31          0.0        0.0   \n",
       "32      AFG      Asia  Afghanistan  2020-02-01          0.0        0.0   \n",
       "33      AFG      Asia  Afghanistan  2020-02-02          0.0        0.0   \n",
       "34      AFG      Asia  Afghanistan  2020-02-03          0.0        0.0   \n",
       "35      AFG      Asia  Afghanistan  2020-02-04          0.0        0.0   \n",
       "36      AFG      Asia  Afghanistan  2020-02-05          0.0        0.0   \n",
       "37      AFG      Asia  Afghanistan  2020-02-06          0.0        0.0   \n",
       "38      AFG      Asia  Afghanistan  2020-02-07          0.0        0.0   \n",
       "39      AFG      Asia  Afghanistan  2020-02-08          0.0        0.0   \n",
       "40      AFG      Asia  Afghanistan  2020-02-09          0.0        0.0   \n",
       "41      AFG      Asia  Afghanistan  2020-02-10          0.0        0.0   \n",
       "42      AFG      Asia  Afghanistan  2020-02-11          0.0        0.0   \n",
       "43      AFG      Asia  Afghanistan  2020-02-12          0.0        0.0   \n",
       "44      AFG      Asia  Afghanistan  2020-02-13          0.0        0.0   \n",
       "45      AFG      Asia  Afghanistan  2020-02-14          0.0        0.0   \n",
       "46      AFG      Asia  Afghanistan  2020-02-15          0.0        0.0   \n",
       "47      AFG      Asia  Afghanistan  2020-02-16          0.0        0.0   \n",
       "48      AFG      Asia  Afghanistan  2020-02-17          0.0        0.0   \n",
       "49      AFG      Asia  Afghanistan  2020-02-18          0.0        0.0   \n",
       "\n",
       "    total_deaths  new_deaths  total_cases_per_million  new_cases_per_million  \\\n",
       "0            0.0         0.0                      0.0                    0.0   \n",
       "1            0.0         0.0                      0.0                    0.0   \n",
       "2            0.0         0.0                      0.0                    0.0   \n",
       "3            0.0         0.0                      0.0                    0.0   \n",
       "4            0.0         0.0                      0.0                    0.0   \n",
       "5            0.0         0.0                      0.0                    0.0   \n",
       "6            0.0         0.0                      0.0                    0.0   \n",
       "7            0.0         0.0                      0.0                    0.0   \n",
       "8            0.0         0.0                      0.0                    0.0   \n",
       "9            0.0         0.0                      0.0                    0.0   \n",
       "10           0.0         0.0                      0.0                    0.0   \n",
       "11           0.0         0.0                      0.0                    0.0   \n",
       "12           0.0         0.0                      0.0                    0.0   \n",
       "13           0.0         0.0                      0.0                    0.0   \n",
       "14           0.0         0.0                      0.0                    0.0   \n",
       "15           0.0         0.0                      0.0                    0.0   \n",
       "16           0.0         0.0                      0.0                    0.0   \n",
       "17           0.0         0.0                      0.0                    0.0   \n",
       "18           0.0         0.0                      0.0                    0.0   \n",
       "19           0.0         0.0                      0.0                    0.0   \n",
       "20           0.0         0.0                      0.0                    0.0   \n",
       "21           0.0         0.0                      0.0                    0.0   \n",
       "22           0.0         0.0                      0.0                    0.0   \n",
       "23           0.0         0.0                      0.0                    0.0   \n",
       "24           0.0         0.0                      0.0                    0.0   \n",
       "25           0.0         0.0                      0.0                    0.0   \n",
       "26           0.0         0.0                      0.0                    0.0   \n",
       "27           0.0         0.0                      0.0                    0.0   \n",
       "28           0.0         0.0                      0.0                    0.0   \n",
       "29           0.0         0.0                      0.0                    0.0   \n",
       "30           0.0         0.0                      0.0                    0.0   \n",
       "31           0.0         0.0                      0.0                    0.0   \n",
       "32           0.0         0.0                      0.0                    0.0   \n",
       "33           0.0         0.0                      0.0                    0.0   \n",
       "34           0.0         0.0                      0.0                    0.0   \n",
       "35           0.0         0.0                      0.0                    0.0   \n",
       "36           0.0         0.0                      0.0                    0.0   \n",
       "37           0.0         0.0                      0.0                    0.0   \n",
       "38           0.0         0.0                      0.0                    0.0   \n",
       "39           0.0         0.0                      0.0                    0.0   \n",
       "40           0.0         0.0                      0.0                    0.0   \n",
       "41           0.0         0.0                      0.0                    0.0   \n",
       "42           0.0         0.0                      0.0                    0.0   \n",
       "43           0.0         0.0                      0.0                    0.0   \n",
       "44           0.0         0.0                      0.0                    0.0   \n",
       "45           0.0         0.0                      0.0                    0.0   \n",
       "46           0.0         0.0                      0.0                    0.0   \n",
       "47           0.0         0.0                      0.0                    0.0   \n",
       "48           0.0         0.0                      0.0                    0.0   \n",
       "49           0.0         0.0                      0.0                    0.0   \n",
       "\n",
       "    ...  aged_70_older  gdp_per_capita  extreme_poverty  \\\n",
       "0   ...          1.337        1803.987              NaN   \n",
       "1   ...          1.337        1803.987              NaN   \n",
       "2   ...          1.337        1803.987              NaN   \n",
       "3   ...          1.337        1803.987              NaN   \n",
       "4   ...          1.337        1803.987              NaN   \n",
       "5   ...          1.337        1803.987              NaN   \n",
       "6   ...          1.337        1803.987              NaN   \n",
       "7   ...          1.337        1803.987              NaN   \n",
       "8   ...          1.337        1803.987              NaN   \n",
       "9   ...          1.337        1803.987              NaN   \n",
       "10  ...          1.337        1803.987              NaN   \n",
       "11  ...          1.337        1803.987              NaN   \n",
       "12  ...          1.337        1803.987              NaN   \n",
       "13  ...          1.337        1803.987              NaN   \n",
       "14  ...          1.337        1803.987              NaN   \n",
       "15  ...          1.337        1803.987              NaN   \n",
       "16  ...          1.337        1803.987              NaN   \n",
       "17  ...          1.337        1803.987              NaN   \n",
       "18  ...          1.337        1803.987              NaN   \n",
       "19  ...          1.337        1803.987              NaN   \n",
       "20  ...          1.337        1803.987              NaN   \n",
       "21  ...          1.337        1803.987              NaN   \n",
       "22  ...          1.337        1803.987              NaN   \n",
       "23  ...          1.337        1803.987              NaN   \n",
       "24  ...          1.337        1803.987              NaN   \n",
       "25  ...          1.337        1803.987              NaN   \n",
       "26  ...          1.337        1803.987              NaN   \n",
       "27  ...          1.337        1803.987              NaN   \n",
       "28  ...          1.337        1803.987              NaN   \n",
       "29  ...          1.337        1803.987              NaN   \n",
       "30  ...          1.337        1803.987              NaN   \n",
       "31  ...          1.337        1803.987              NaN   \n",
       "32  ...          1.337        1803.987              NaN   \n",
       "33  ...          1.337        1803.987              NaN   \n",
       "34  ...          1.337        1803.987              NaN   \n",
       "35  ...          1.337        1803.987              NaN   \n",
       "36  ...          1.337        1803.987              NaN   \n",
       "37  ...          1.337        1803.987              NaN   \n",
       "38  ...          1.337        1803.987              NaN   \n",
       "39  ...          1.337        1803.987              NaN   \n",
       "40  ...          1.337        1803.987              NaN   \n",
       "41  ...          1.337        1803.987              NaN   \n",
       "42  ...          1.337        1803.987              NaN   \n",
       "43  ...          1.337        1803.987              NaN   \n",
       "44  ...          1.337        1803.987              NaN   \n",
       "45  ...          1.337        1803.987              NaN   \n",
       "46  ...          1.337        1803.987              NaN   \n",
       "47  ...          1.337        1803.987              NaN   \n",
       "48  ...          1.337        1803.987              NaN   \n",
       "49  ...          1.337        1803.987              NaN   \n",
       "\n",
       "    cardiovasc_death_rate  diabetes_prevalence  female_smokers  male_smokers  \\\n",
       "0                 597.029                 9.59             NaN           NaN   \n",
       "1                 597.029                 9.59             NaN           NaN   \n",
       "2                 597.029                 9.59             NaN           NaN   \n",
       "3                 597.029                 9.59             NaN           NaN   \n",
       "4                 597.029                 9.59             NaN           NaN   \n",
       "5                 597.029                 9.59             NaN           NaN   \n",
       "6                 597.029                 9.59             NaN           NaN   \n",
       "7                 597.029                 9.59             NaN           NaN   \n",
       "8                 597.029                 9.59             NaN           NaN   \n",
       "9                 597.029                 9.59             NaN           NaN   \n",
       "10                597.029                 9.59             NaN           NaN   \n",
       "11                597.029                 9.59             NaN           NaN   \n",
       "12                597.029                 9.59             NaN           NaN   \n",
       "13                597.029                 9.59             NaN           NaN   \n",
       "14                597.029                 9.59             NaN           NaN   \n",
       "15                597.029                 9.59             NaN           NaN   \n",
       "16                597.029                 9.59             NaN           NaN   \n",
       "17                597.029                 9.59             NaN           NaN   \n",
       "18                597.029                 9.59             NaN           NaN   \n",
       "19                597.029                 9.59             NaN           NaN   \n",
       "20                597.029                 9.59             NaN           NaN   \n",
       "21                597.029                 9.59             NaN           NaN   \n",
       "22                597.029                 9.59             NaN           NaN   \n",
       "23                597.029                 9.59             NaN           NaN   \n",
       "24                597.029                 9.59             NaN           NaN   \n",
       "25                597.029                 9.59             NaN           NaN   \n",
       "26                597.029                 9.59             NaN           NaN   \n",
       "27                597.029                 9.59             NaN           NaN   \n",
       "28                597.029                 9.59             NaN           NaN   \n",
       "29                597.029                 9.59             NaN           NaN   \n",
       "30                597.029                 9.59             NaN           NaN   \n",
       "31                597.029                 9.59             NaN           NaN   \n",
       "32                597.029                 9.59             NaN           NaN   \n",
       "33                597.029                 9.59             NaN           NaN   \n",
       "34                597.029                 9.59             NaN           NaN   \n",
       "35                597.029                 9.59             NaN           NaN   \n",
       "36                597.029                 9.59             NaN           NaN   \n",
       "37                597.029                 9.59             NaN           NaN   \n",
       "38                597.029                 9.59             NaN           NaN   \n",
       "39                597.029                 9.59             NaN           NaN   \n",
       "40                597.029                 9.59             NaN           NaN   \n",
       "41                597.029                 9.59             NaN           NaN   \n",
       "42                597.029                 9.59             NaN           NaN   \n",
       "43                597.029                 9.59             NaN           NaN   \n",
       "44                597.029                 9.59             NaN           NaN   \n",
       "45                597.029                 9.59             NaN           NaN   \n",
       "46                597.029                 9.59             NaN           NaN   \n",
       "47                597.029                 9.59             NaN           NaN   \n",
       "48                597.029                 9.59             NaN           NaN   \n",
       "49                597.029                 9.59             NaN           NaN   \n",
       "\n",
       "    handwashing_facilities  hospital_beds_per_thousand  life_expectancy  \n",
       "0                   37.746                         0.5            64.83  \n",
       "1                   37.746                         0.5            64.83  \n",
       "2                   37.746                         0.5            64.83  \n",
       "3                   37.746                         0.5            64.83  \n",
       "4                   37.746                         0.5            64.83  \n",
       "5                   37.746                         0.5            64.83  \n",
       "6                   37.746                         0.5            64.83  \n",
       "7                   37.746                         0.5            64.83  \n",
       "8                   37.746                         0.5            64.83  \n",
       "9                   37.746                         0.5            64.83  \n",
       "10                  37.746                         0.5            64.83  \n",
       "11                  37.746                         0.5            64.83  \n",
       "12                  37.746                         0.5            64.83  \n",
       "13                  37.746                         0.5            64.83  \n",
       "14                  37.746                         0.5            64.83  \n",
       "15                  37.746                         0.5            64.83  \n",
       "16                  37.746                         0.5            64.83  \n",
       "17                  37.746                         0.5            64.83  \n",
       "18                  37.746                         0.5            64.83  \n",
       "19                  37.746                         0.5            64.83  \n",
       "20                  37.746                         0.5            64.83  \n",
       "21                  37.746                         0.5            64.83  \n",
       "22                  37.746                         0.5            64.83  \n",
       "23                  37.746                         0.5            64.83  \n",
       "24                  37.746                         0.5            64.83  \n",
       "25                  37.746                         0.5            64.83  \n",
       "26                  37.746                         0.5            64.83  \n",
       "27                  37.746                         0.5            64.83  \n",
       "28                  37.746                         0.5            64.83  \n",
       "29                  37.746                         0.5            64.83  \n",
       "30                  37.746                         0.5            64.83  \n",
       "31                  37.746                         0.5            64.83  \n",
       "32                  37.746                         0.5            64.83  \n",
       "33                  37.746                         0.5            64.83  \n",
       "34                  37.746                         0.5            64.83  \n",
       "35                  37.746                         0.5            64.83  \n",
       "36                  37.746                         0.5            64.83  \n",
       "37                  37.746                         0.5            64.83  \n",
       "38                  37.746                         0.5            64.83  \n",
       "39                  37.746                         0.5            64.83  \n",
       "40                  37.746                         0.5            64.83  \n",
       "41                  37.746                         0.5            64.83  \n",
       "42                  37.746                         0.5            64.83  \n",
       "43                  37.746                         0.5            64.83  \n",
       "44                  37.746                         0.5            64.83  \n",
       "45                  37.746                         0.5            64.83  \n",
       "46                  37.746                         0.5            64.83  \n",
       "47                  37.746                         0.5            64.83  \n",
       "48                  37.746                         0.5            64.83  \n",
       "49                  37.746                         0.5            64.83  \n",
       "\n",
       "[50 rows x 36 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv('C:/Users/pc lenevo/Desktop/covid_3scenario/data_confinement/owid-covid-data.csv') # having First.csv zipped file.\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date= pd.to_datetime(df['date'],format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['new_cases','total_cases','total_deaths','new_deaths', 'population','date','location']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(df.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['month'] = df['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.astype(float)\n",
    "#df['day'] = df.index.weekday\n",
    "#df['week'] = df.index.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-03-02'\n",
    "end_date = '2020-06-02'\n",
    "start_date2 = '2020-06-03'\n",
    "end_date2 = '2020-06-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = (covid_data['date'] >= start_date) & (covid_data['date'] <= end_date) &  (covid_data['location'].isin(['Morocco']))\n",
    "mask1 = (df['date'] >= start_date) & (df['date'] <= end_date) &  (df['location'].isin(['Morocco']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask2 = (df['date'] > start_date2) & (df['date'] <= end_date2) &  (df['location'].isin(['Morocco']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['location'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df.loc[mask1]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = df.loc[mask2]\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\The Crown\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\The Crown\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X_test['timestamp'] = pd.to_datetime(X_test['date'], infer_datetime_format=True)\n",
    "X_test.set_index('timestamp', inplace=True)\n",
    "\n",
    "X_train['timestamp'] = pd.to_datetime(X_train['date'], infer_datetime_format=True)\n",
    "X_train.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train[['new_cases','total_cases','total_deaths', 'new_deaths', 'population']]\n",
    "\n",
    "X_test=X_test[['new_cases','total_cases','total_deaths', 'new_deaths', 'population']]\n",
    "Y_test=X_test[['new_cases']]\n",
    "\n",
    "Y_train=X_train[['new_cases']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "days=X_test.index\n",
    "days=days.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_seq=1\n",
    "days=days[nb_seq:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93, 5)\n",
      "(93, 1)\n",
      "(8, 5)\n",
      "(8, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler2= MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_col=5\n",
    "nb_seq=1\n",
    "n_out=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)   var1(t)   var2(t)  \\\n",
      "1    0.024911   0.078396   0.048780   0.000000        0.0  0.024911  0.078396   \n",
      "2    0.024911   0.078396   0.048780   0.000000        0.0  0.010676  0.000511   \n",
      "3    0.010676   0.000511   0.004878   0.066667        0.0  0.003559  0.000638   \n",
      "4    0.003559   0.000638   0.004878   0.000000        0.0  0.003559  0.000766   \n",
      "5    0.003559   0.000766   0.004878   0.000000        0.0  0.039146  0.002171   \n",
      "..        ...        ...        ...        ...        ...       ...       ...   \n",
      "82   0.494662   0.497446   0.775610   0.066667        0.0  0.498221  0.708248   \n",
      "83   0.498221   0.708248   0.892683   0.000000        0.0  0.672598  0.690373   \n",
      "84   0.672598   0.690373   0.892683   0.133333        0.0  0.626334  0.666241   \n",
      "85   0.626334   0.666241   0.882927   0.133333        0.0  0.619217  0.625894   \n",
      "86   0.619217   0.625894   0.848781   0.066667        0.0  0.498221  0.643769   \n",
      "\n",
      "     var3(t)   var4(t)  var5(t)  ...  var1(t+5)  var2(t+5)  var3(t+5)  \\\n",
      "1   0.048780  0.000000      0.0  ...   0.035587   0.003447   0.004878   \n",
      "2   0.004878  0.066667      0.0  ...   0.032028   0.004597   0.004878   \n",
      "3   0.004878  0.000000      0.0  ...   0.024911   0.005490   0.009756   \n",
      "4   0.004878  0.000000      0.0  ...   0.035587   0.006767   0.009756   \n",
      "5   0.004878  0.000000      0.0  ...   0.032028   0.007916   0.009756   \n",
      "..       ...       ...      ...  ...        ...        ...        ...   \n",
      "82  0.892683  0.000000      0.0  ...   0.519573   0.583248   0.834146   \n",
      "83  0.892683  0.133333      0.0  ...   0.362989   0.564607   0.829268   \n",
      "84  0.882927  0.133333      0.0  ...   0.245552   0.551583   0.819512   \n",
      "85  0.848781  0.066667      0.0  ...   0.469751   0.542773   0.804878   \n",
      "86  0.873171  0.333333      0.0  ...   0.195730   0.525919   0.790244   \n",
      "\n",
      "    var4(t+5)  var5(t+5)  var1(t+6)  var2(t+6)  var3(t+6)  var4(t+6)  \\\n",
      "1    0.000000        0.0   0.032028   0.004597   0.004878   0.000000   \n",
      "2    0.000000        0.0   0.024911   0.005490   0.009756   0.066667   \n",
      "3    0.066667        0.0   0.035587   0.006767   0.009756   0.000000   \n",
      "4    0.000000        0.0   0.032028   0.007916   0.009756   0.000000   \n",
      "5    0.000000        0.0   0.035587   0.012130   0.014634   0.000000   \n",
      "..        ...        ...        ...        ...        ...        ...   \n",
      "82   0.066667        0.0   0.362989   0.564607   0.829268   0.133333   \n",
      "83   0.133333        0.0   0.245552   0.551583   0.819512   0.200000   \n",
      "84   0.200000        0.0   0.469751   0.542773   0.804878   0.200000   \n",
      "85   0.200000        0.0   0.195730   0.525919   0.790244   0.066667   \n",
      "86   0.066667        0.0   0.569395   0.603677   0.843902   0.133333   \n",
      "\n",
      "    var5(t+6)  \n",
      "1         0.0  \n",
      "2         0.0  \n",
      "3         0.0  \n",
      "4         0.0  \n",
      "5         0.0  \n",
      "..        ...  \n",
      "82        0.0  \n",
      "83        0.0  \n",
      "84        0.0  \n",
      "85        0.0  \n",
      "86        0.0  \n",
      "\n",
      "[86 rows x 40 columns]\n",
      "(86, 40)\n",
      "(1, 40)\n",
      "(86, 8)\n",
      "(1, 8)\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import GRU, LSTM, CuDNNGRU, CuDNNLSTM, Activation\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "X_train=X_train.values\n",
    "X_test=X_test.values\n",
    "Y_train=Y_train.values\n",
    "Y_test=Y_test.values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ensure all data is float\n",
    "X_train = X_train.astype('float32')\n",
    "# ensure all data is float\n",
    "X_test = X_test.astype('float32')\n",
    "# ensure all data is float\n",
    "Y_test = Y_test.astype('float32')\n",
    "# ensure all data is float\n",
    "Y_train = Y_train.astype('float32')\n",
    "\n",
    "# normalize features\n",
    "\n",
    "\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test= scaler.fit_transform(X_test)\n",
    "Y_train= scaler2.fit_transform(Y_train)\n",
    "Y_test= scaler2.fit_transform(Y_test)\n",
    "\n",
    "# reframe as supervised learning\n",
    "X_train = series_to_supervised(X_train, nb_seq, n_out)\n",
    "X_test = series_to_supervised(X_test,  nb_seq, n_out)\n",
    "print(X_train)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "Y_train = series_to_supervised(Y_train,  nb_seq, n_out)\n",
    "Y_test = series_to_supervised(Y_test,  nb_seq, n_out)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var4(t-1)</th>\n",
       "      <th>var5(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var2(t)</th>\n",
       "      <th>var3(t)</th>\n",
       "      <th>var4(t)</th>\n",
       "      <th>var5(t)</th>\n",
       "      <th>...</th>\n",
       "      <th>var1(t+5)</th>\n",
       "      <th>var2(t+5)</th>\n",
       "      <th>var3(t+5)</th>\n",
       "      <th>var4(t+5)</th>\n",
       "      <th>var5(t+5)</th>\n",
       "      <th>var1(t+6)</th>\n",
       "      <th>var2(t+6)</th>\n",
       "      <th>var3(t+6)</th>\n",
       "      <th>var4(t+6)</th>\n",
       "      <th>var5(t+6)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.078396</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.078396</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035587</td>\n",
       "      <td>0.003447</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.078396</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010676</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.005490</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010676</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.005490</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035587</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035587</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035587</td>\n",
       "      <td>0.012130</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.494662</td>\n",
       "      <td>0.497446</td>\n",
       "      <td>0.775610</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498221</td>\n",
       "      <td>0.708248</td>\n",
       "      <td>0.892683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519573</td>\n",
       "      <td>0.583248</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.362989</td>\n",
       "      <td>0.564607</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.498221</td>\n",
       "      <td>0.708248</td>\n",
       "      <td>0.892683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.672598</td>\n",
       "      <td>0.690373</td>\n",
       "      <td>0.892683</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362989</td>\n",
       "      <td>0.564607</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.245552</td>\n",
       "      <td>0.551583</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.672598</td>\n",
       "      <td>0.690373</td>\n",
       "      <td>0.892683</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626334</td>\n",
       "      <td>0.666241</td>\n",
       "      <td>0.882927</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245552</td>\n",
       "      <td>0.551583</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469751</td>\n",
       "      <td>0.542773</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.626334</td>\n",
       "      <td>0.666241</td>\n",
       "      <td>0.882927</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619217</td>\n",
       "      <td>0.625894</td>\n",
       "      <td>0.848781</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469751</td>\n",
       "      <td>0.542773</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.195730</td>\n",
       "      <td>0.525919</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.619217</td>\n",
       "      <td>0.625894</td>\n",
       "      <td>0.848781</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498221</td>\n",
       "      <td>0.643769</td>\n",
       "      <td>0.873171</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195730</td>\n",
       "      <td>0.525919</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.569395</td>\n",
       "      <td>0.603677</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)   var1(t)   var2(t)  \\\n",
       "1    0.024911   0.078396   0.048780   0.000000        0.0  0.024911  0.078396   \n",
       "2    0.024911   0.078396   0.048780   0.000000        0.0  0.010676  0.000511   \n",
       "3    0.010676   0.000511   0.004878   0.066667        0.0  0.003559  0.000638   \n",
       "4    0.003559   0.000638   0.004878   0.000000        0.0  0.003559  0.000766   \n",
       "5    0.003559   0.000766   0.004878   0.000000        0.0  0.039146  0.002171   \n",
       "..        ...        ...        ...        ...        ...       ...       ...   \n",
       "82   0.494662   0.497446   0.775610   0.066667        0.0  0.498221  0.708248   \n",
       "83   0.498221   0.708248   0.892683   0.000000        0.0  0.672598  0.690373   \n",
       "84   0.672598   0.690373   0.892683   0.133333        0.0  0.626334  0.666241   \n",
       "85   0.626334   0.666241   0.882927   0.133333        0.0  0.619217  0.625894   \n",
       "86   0.619217   0.625894   0.848781   0.066667        0.0  0.498221  0.643769   \n",
       "\n",
       "     var3(t)   var4(t)  var5(t)  ...  var1(t+5)  var2(t+5)  var3(t+5)  \\\n",
       "1   0.048780  0.000000      0.0  ...   0.035587   0.003447   0.004878   \n",
       "2   0.004878  0.066667      0.0  ...   0.032028   0.004597   0.004878   \n",
       "3   0.004878  0.000000      0.0  ...   0.024911   0.005490   0.009756   \n",
       "4   0.004878  0.000000      0.0  ...   0.035587   0.006767   0.009756   \n",
       "5   0.004878  0.000000      0.0  ...   0.032028   0.007916   0.009756   \n",
       "..       ...       ...      ...  ...        ...        ...        ...   \n",
       "82  0.892683  0.000000      0.0  ...   0.519573   0.583248   0.834146   \n",
       "83  0.892683  0.133333      0.0  ...   0.362989   0.564607   0.829268   \n",
       "84  0.882927  0.133333      0.0  ...   0.245552   0.551583   0.819512   \n",
       "85  0.848781  0.066667      0.0  ...   0.469751   0.542773   0.804878   \n",
       "86  0.873171  0.333333      0.0  ...   0.195730   0.525919   0.790244   \n",
       "\n",
       "    var4(t+5)  var5(t+5)  var1(t+6)  var2(t+6)  var3(t+6)  var4(t+6)  \\\n",
       "1    0.000000        0.0   0.032028   0.004597   0.004878   0.000000   \n",
       "2    0.000000        0.0   0.024911   0.005490   0.009756   0.066667   \n",
       "3    0.066667        0.0   0.035587   0.006767   0.009756   0.000000   \n",
       "4    0.000000        0.0   0.032028   0.007916   0.009756   0.000000   \n",
       "5    0.000000        0.0   0.035587   0.012130   0.014634   0.000000   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "82   0.066667        0.0   0.362989   0.564607   0.829268   0.133333   \n",
       "83   0.133333        0.0   0.245552   0.551583   0.819512   0.200000   \n",
       "84   0.200000        0.0   0.469751   0.542773   0.804878   0.200000   \n",
       "85   0.200000        0.0   0.195730   0.525919   0.790244   0.066667   \n",
       "86   0.066667        0.0   0.569395   0.603677   0.843902   0.133333   \n",
       "\n",
       "    var5(t+6)  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "5         0.0  \n",
       "..        ...  \n",
       "82        0.0  \n",
       "83        0.0  \n",
       "84        0.0  \n",
       "85        0.0  \n",
       "86        0.0  \n",
       "\n",
       "[86 rows x 40 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_y_need_train=X_train.columns[[5,10,15,20,25,30,35]]\n",
    "col_y_need_test=X_test.columns[[5,10,15,20,25,30,35]]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 7)\n",
      "(1, 7)\n"
     ]
    }
   ],
   "source": [
    "# drop columns we don't want to predict\n",
    "Y_train = X_train[col_y_need_train]\n",
    "Y_test=X_test[col_y_need_test]\n",
    "Y_train\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t)  var1(t+1)  var1(t+2)  var1(t+3)  var1(t+4)  var1(t+5)  var1(t+6)\n",
      "1   0.024911   0.010676   0.003559   0.003559   0.039146   0.035587   0.032028\n",
      "2   0.010676   0.003559   0.003559   0.039146   0.035587   0.032028   0.024911\n",
      "3   0.003559   0.003559   0.039146   0.035587   0.032028   0.024911   0.035587\n",
      "4   0.003559   0.039146   0.035587   0.032028   0.024911   0.035587   0.032028\n",
      "5   0.039146   0.035587   0.032028   0.024911   0.035587   0.032028   0.035587\n",
      "..       ...        ...        ...        ...        ...        ...        ...\n",
      "82  0.498221   0.672598   0.626334   0.619217   0.498221   0.519573   0.362989\n",
      "83  0.672598   0.626334   0.619217   0.498221   0.519573   0.362989   0.245552\n",
      "84  0.626334   0.619217   0.498221   0.519573   0.362989   0.245552   0.469751\n",
      "85  0.619217   0.498221   0.519573   0.362989   0.245552   0.469751   0.195730\n",
      "86  0.498221   0.519573   0.362989   0.245552   0.469751   0.195730   0.569395\n",
      "\n",
      "[86 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_y_need_trainX=X_train.columns[[0,1,2,3,4]]\n",
    "col_y_need_testX=X_test.columns[[0,1,2,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 5)\n",
      "(1, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var4(t-1)</th>\n",
       "      <th>var5(t-1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.078396</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.078396</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010676</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.494662</td>\n",
       "      <td>0.497446</td>\n",
       "      <td>0.775610</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.498221</td>\n",
       "      <td>0.708248</td>\n",
       "      <td>0.892683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.672598</td>\n",
       "      <td>0.690373</td>\n",
       "      <td>0.892683</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.626334</td>\n",
       "      <td>0.666241</td>\n",
       "      <td>0.882927</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.619217</td>\n",
       "      <td>0.625894</td>\n",
       "      <td>0.848781</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)\n",
       "1    0.024911   0.078396   0.048780   0.000000        0.0\n",
       "2    0.024911   0.078396   0.048780   0.000000        0.0\n",
       "3    0.010676   0.000511   0.004878   0.066667        0.0\n",
       "4    0.003559   0.000638   0.004878   0.000000        0.0\n",
       "5    0.003559   0.000766   0.004878   0.000000        0.0\n",
       "..        ...        ...        ...        ...        ...\n",
       "82   0.494662   0.497446   0.775610   0.066667        0.0\n",
       "83   0.498221   0.708248   0.892683   0.000000        0.0\n",
       "84   0.672598   0.690373   0.892683   0.133333        0.0\n",
       "85   0.626334   0.666241   0.882927   0.133333        0.0\n",
       "86   0.619217   0.625894   0.848781   0.066667        0.0\n",
       "\n",
       "[86 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop columns we don't want to predict\n",
    "X_train = X_train[col_y_need_trainX]\n",
    "X_test=X_test[col_y_need_testX]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 5) (1, 7) (86, 1, 5) (86, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train=X_train.values\n",
    "X_test=X_test.values\n",
    "\n",
    "Y_train=Y_train.values\n",
    "Y_test=Y_test.values\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "X_test = X_test.reshape((X_test.shape[0], nb_seq, nb_col))\n",
    "X_train = X_train.reshape((X_train.shape[0], nb_seq, nb_col))\n",
    "\n",
    "print(X_test.shape, Y_test.shape, X_train.shape, Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5) (1, 7) (86, 5) (86, 7)\n"
     ]
    }
   ],
   "source": [
    "n_input = X_train.shape[1] * X_train.shape[2]\n",
    "X_train= X_train.reshape((X_train.shape[0], n_input))\n",
    "n_input2 = X_test.shape[1] * X_test.shape[2]\n",
    "X_test= X_test.reshape((X_test.shape[0], n_input2))\n",
    "\n",
    "print(X_test.shape, Y_test.shape, X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT ML Regressor\n",
    "from sklearn.linear_model import LinearRegression # Linear regression\n",
    "from sklearn.ensemble import RandomForestRegressor # random forest regression\n",
    "from sklearn.neural_network import MLPRegressor # neural network regression\n",
    "from sklearn.svm import SVR # support vector regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 0.1403 - accuracy: 0.1512 - val_loss: 0.1716 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1327 - accuracy: 0.1512 - val_loss: 0.1617 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1256 - accuracy: 0.1628 - val_loss: 0.1527 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1189 - accuracy: 0.1512 - val_loss: 0.1448 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1126 - accuracy: 0.1744 - val_loss: 0.1378 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1067 - accuracy: 0.1744 - val_loss: 0.1316 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1013 - accuracy: 0.1512 - val_loss: 0.1264 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0962 - accuracy: 0.1512 - val_loss: 0.1218 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0916 - accuracy: 0.1279 - val_loss: 0.1179 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0873 - accuracy: 0.1279 - val_loss: 0.1146 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0834 - accuracy: 0.1279 - val_loss: 0.1117 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0799 - accuracy: 0.1279 - val_loss: 0.1093 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0766 - accuracy: 0.1163 - val_loss: 0.1073 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0737 - accuracy: 0.1163 - val_loss: 0.1055 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0711 - accuracy: 0.1047 - val_loss: 0.1039 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0687 - accuracy: 0.0930 - val_loss: 0.1025 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0665 - accuracy: 0.0930 - val_loss: 0.1013 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/1500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0646 - accuracy: 0.1047 - val_loss: 0.1001 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0628 - accuracy: 0.1163 - val_loss: 0.0989 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0612 - accuracy: 0.1163 - val_loss: 0.0978 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/1500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0598 - accuracy: 0.1163 - val_loss: 0.0967 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0585 - accuracy: 0.1163 - val_loss: 0.0956 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0573 - accuracy: 0.1279 - val_loss: 0.0945 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0562 - accuracy: 0.1395 - val_loss: 0.0934 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/1500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0552 - accuracy: 0.1395 - val_loss: 0.0923 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0542 - accuracy: 0.1512 - val_loss: 0.0911 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0533 - accuracy: 0.1395 - val_loss: 0.0900 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0525 - accuracy: 0.1628 - val_loss: 0.0888 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0517 - accuracy: 0.1628 - val_loss: 0.0877 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0510 - accuracy: 0.1744 - val_loss: 0.0866 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0503 - accuracy: 0.1744 - val_loss: 0.0856 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0497 - accuracy: 0.1628 - val_loss: 0.0846 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/1500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0491 - accuracy: 0.1628 - val_loss: 0.0836 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0485 - accuracy: 0.1628 - val_loss: 0.0828 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0479 - accuracy: 0.1744 - val_loss: 0.0820 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0474 - accuracy: 0.1744 - val_loss: 0.0812 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0469 - accuracy: 0.1744 - val_loss: 0.0806 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0465 - accuracy: 0.1628 - val_loss: 0.0800 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0460 - accuracy: 0.1628 - val_loss: 0.0795 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0456 - accuracy: 0.1512 - val_loss: 0.0791 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0452 - accuracy: 0.1395 - val_loss: 0.0788 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0448 - accuracy: 0.1395 - val_loss: 0.0785 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0445 - accuracy: 0.1395 - val_loss: 0.0783 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0442 - accuracy: 0.1395 - val_loss: 0.0781 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/1500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0439 - accuracy: 0.1395 - val_loss: 0.0780 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0436 - accuracy: 0.1395 - val_loss: 0.0779 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0433 - accuracy: 0.1279 - val_loss: 0.0778 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0431 - accuracy: 0.1395 - val_loss: 0.0777 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0428 - accuracy: 0.1512 - val_loss: 0.0776 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0426 - accuracy: 0.1628 - val_loss: 0.0775 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0424 - accuracy: 0.1395 - val_loss: 0.0774 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0422 - accuracy: 0.1163 - val_loss: 0.0772 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0420 - accuracy: 0.1163 - val_loss: 0.0770 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0418 - accuracy: 0.1163 - val_loss: 0.0768 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0417 - accuracy: 0.1163 - val_loss: 0.0765 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0415 - accuracy: 0.1279 - val_loss: 0.0762 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0414 - accuracy: 0.1279 - val_loss: 0.0758 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0412 - accuracy: 0.1279 - val_loss: 0.0754 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0411 - accuracy: 0.1279 - val_loss: 0.0749 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0409 - accuracy: 0.1279 - val_loss: 0.0744 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0408 - accuracy: 0.1163 - val_loss: 0.0739 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0407 - accuracy: 0.1163 - val_loss: 0.0733 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0406 - accuracy: 0.1163 - val_loss: 0.0727 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0404 - accuracy: 0.1395 - val_loss: 0.0722 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0403 - accuracy: 0.1512 - val_loss: 0.0716 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0402 - accuracy: 0.1512 - val_loss: 0.0710 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0401 - accuracy: 0.1512 - val_loss: 0.0704 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0400 - accuracy: 0.1628 - val_loss: 0.0699 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0399 - accuracy: 0.1628 - val_loss: 0.0693 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0398 - accuracy: 0.1628 - val_loss: 0.0688 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0397 - accuracy: 0.1744 - val_loss: 0.0683 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0396 - accuracy: 0.1744 - val_loss: 0.0678 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0395 - accuracy: 0.1860 - val_loss: 0.0674 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0394 - accuracy: 0.1860 - val_loss: 0.0670 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0393 - accuracy: 0.1860 - val_loss: 0.0666 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/1500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0392 - accuracy: 0.1860 - val_loss: 0.0662 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/1500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0391 - accuracy: 0.1860 - val_loss: 0.0659 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/1500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0390 - accuracy: 0.1977 - val_loss: 0.0655 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0389 - accuracy: 0.1977 - val_loss: 0.0652 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/1500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0389 - accuracy: 0.2093 - val_loss: 0.0649 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0388 - accuracy: 0.2093 - val_loss: 0.0646 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/1500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0387 - accuracy: 0.2093 - val_loss: 0.0643 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0386 - accuracy: 0.2093 - val_loss: 0.0640 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/1500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0385 - accuracy: 0.1977 - val_loss: 0.0637 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0385 - accuracy: 0.1977 - val_loss: 0.0634 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0384 - accuracy: 0.1977 - val_loss: 0.0631 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/1500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0383 - accuracy: 0.1977 - val_loss: 0.0629 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/1500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0382 - accuracy: 0.1977 - val_loss: 0.0626 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0382 - accuracy: 0.1860 - val_loss: 0.0622 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/1500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0381 - accuracy: 0.1860 - val_loss: 0.0619 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0380 - accuracy: 0.1860 - val_loss: 0.0616 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0380 - accuracy: 0.1860 - val_loss: 0.0613 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0379 - accuracy: 0.1744 - val_loss: 0.0610 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0378 - accuracy: 0.1744 - val_loss: 0.0607 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0378 - accuracy: 0.1744 - val_loss: 0.0605 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/1500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0377 - accuracy: 0.1744 - val_loss: 0.0602 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0376 - accuracy: 0.1860 - val_loss: 0.0599 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/1500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0376 - accuracy: 0.1860 - val_loss: 0.0597 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0375 - accuracy: 0.1860 - val_loss: 0.0594 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0374 - accuracy: 0.1860 - val_loss: 0.0592 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0374 - accuracy: 0.1860 - val_loss: 0.0590 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/1500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0373 - accuracy: 0.1860 - val_loss: 0.0587 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0373 - accuracy: 0.1977 - val_loss: 0.0585 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0372 - accuracy: 0.1977 - val_loss: 0.0584 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0371 - accuracy: 0.1977 - val_loss: 0.0582 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/1500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0371 - accuracy: 0.1977 - val_loss: 0.0580 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0370 - accuracy: 0.1977 - val_loss: 0.0579 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0370 - accuracy: 0.1977 - val_loss: 0.0577 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0369 - accuracy: 0.1977 - val_loss: 0.0575 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/1500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0368 - accuracy: 0.1977 - val_loss: 0.0574 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/1500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0368 - accuracy: 0.1977 - val_loss: 0.0573 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0367 - accuracy: 0.1977 - val_loss: 0.0571 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0367 - accuracy: 0.1977 - val_loss: 0.0570 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0366 - accuracy: 0.1860 - val_loss: 0.0568 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/1500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0366 - accuracy: 0.1860 - val_loss: 0.0567 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0365 - accuracy: 0.1860 - val_loss: 0.0566 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0365 - accuracy: 0.1860 - val_loss: 0.0564 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0364 - accuracy: 0.1860 - val_loss: 0.0563 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0364 - accuracy: 0.1860 - val_loss: 0.0561 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0363 - accuracy: 0.1860 - val_loss: 0.0560 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0363 - accuracy: 0.1977 - val_loss: 0.0559 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0362 - accuracy: 0.1860 - val_loss: 0.0557 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0362 - accuracy: 0.1744 - val_loss: 0.0556 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0361 - accuracy: 0.1744 - val_loss: 0.0554 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0361 - accuracy: 0.1744 - val_loss: 0.0553 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/1500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0360 - accuracy: 0.1628 - val_loss: 0.0552 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/1500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0360 - accuracy: 0.1628 - val_loss: 0.0550 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0359 - accuracy: 0.1628 - val_loss: 0.0549 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/1500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0359 - accuracy: 0.1628 - val_loss: 0.0548 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0358 - accuracy: 0.1628 - val_loss: 0.0546 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0358 - accuracy: 0.1628 - val_loss: 0.0545 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0357 - accuracy: 0.1628 - val_loss: 0.0544 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0357 - accuracy: 0.1628 - val_loss: 0.0543 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0356 - accuracy: 0.1628 - val_loss: 0.0542 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0356 - accuracy: 0.1628 - val_loss: 0.0540 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0355 - accuracy: 0.1628 - val_loss: 0.0539 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0355 - accuracy: 0.1628 - val_loss: 0.0538 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0355 - accuracy: 0.1628 - val_loss: 0.0537 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0354 - accuracy: 0.1628 - val_loss: 0.0536 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0354 - accuracy: 0.1628 - val_loss: 0.0534 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0353 - accuracy: 0.1628 - val_loss: 0.0533 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0353 - accuracy: 0.1628 - val_loss: 0.0532 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/1500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0352 - accuracy: 0.1628 - val_loss: 0.0531 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/1500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0352 - accuracy: 0.1512 - val_loss: 0.0530 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/1500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0351 - accuracy: 0.1512 - val_loss: 0.0528 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/1500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0351 - accuracy: 0.1512 - val_loss: 0.0527 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0351 - accuracy: 0.1512 - val_loss: 0.0526 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0350 - accuracy: 0.1512 - val_loss: 0.0525 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0350 - accuracy: 0.1628 - val_loss: 0.0524 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/1500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0349 - accuracy: 0.1744 - val_loss: 0.0523 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0349 - accuracy: 0.1744 - val_loss: 0.0521 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0349 - accuracy: 0.1744 - val_loss: 0.0520 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0348 - accuracy: 0.1744 - val_loss: 0.0519 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0348 - accuracy: 0.1744 - val_loss: 0.0518 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0347 - accuracy: 0.1744 - val_loss: 0.0517 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0347 - accuracy: 0.1744 - val_loss: 0.0515 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0347 - accuracy: 0.1744 - val_loss: 0.0514 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0346 - accuracy: 0.1744 - val_loss: 0.0513 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0346 - accuracy: 0.1744 - val_loss: 0.0512 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0345 - accuracy: 0.1744 - val_loss: 0.0511 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0345 - accuracy: 0.1744 - val_loss: 0.0509 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0345 - accuracy: 0.1744 - val_loss: 0.0508 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/1500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0344 - accuracy: 0.1744 - val_loss: 0.0507 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0344 - accuracy: 0.1744 - val_loss: 0.0506 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0343 - accuracy: 0.1744 - val_loss: 0.0504 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0343 - accuracy: 0.1744 - val_loss: 0.0503 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0343 - accuracy: 0.1744 - val_loss: 0.0502 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0342 - accuracy: 0.1744 - val_loss: 0.0500 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/1500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0342 - accuracy: 0.1744 - val_loss: 0.0499 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/1500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0342 - accuracy: 0.1744 - val_loss: 0.0498 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0341 - accuracy: 0.1744 - val_loss: 0.0496 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0341 - accuracy: 0.1744 - val_loss: 0.0495 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0340 - accuracy: 0.1744 - val_loss: 0.0494 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0340 - accuracy: 0.1744 - val_loss: 0.0492 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/1500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0340 - accuracy: 0.1744 - val_loss: 0.0491 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/1500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0339 - accuracy: 0.1744 - val_loss: 0.0490 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/1500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0339 - accuracy: 0.1744 - val_loss: 0.0488 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0339 - accuracy: 0.1744 - val_loss: 0.0487 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0338 - accuracy: 0.1744 - val_loss: 0.0485 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0338 - accuracy: 0.1744 - val_loss: 0.0484 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/1500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0338 - accuracy: 0.1744 - val_loss: 0.0483 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0337 - accuracy: 0.1628 - val_loss: 0.0481 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/1500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0337 - accuracy: 0.1628 - val_loss: 0.0480 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0337 - accuracy: 0.1628 - val_loss: 0.0478 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/1500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0336 - accuracy: 0.1744 - val_loss: 0.0477 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0336 - accuracy: 0.1744 - val_loss: 0.0475 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0336 - accuracy: 0.1744 - val_loss: 0.0474 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0335 - accuracy: 0.1744 - val_loss: 0.0472 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0335 - accuracy: 0.1744 - val_loss: 0.0471 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0335 - accuracy: 0.1744 - val_loss: 0.0470 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/1500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0334 - accuracy: 0.1744 - val_loss: 0.0468 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0334 - accuracy: 0.1744 - val_loss: 0.0467 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0334 - accuracy: 0.1744 - val_loss: 0.0465 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0333 - accuracy: 0.1744 - val_loss: 0.0463 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/1500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0333 - accuracy: 0.1744 - val_loss: 0.0462 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0333 - accuracy: 0.1744 - val_loss: 0.0460 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0332 - accuracy: 0.1744 - val_loss: 0.0459 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0332 - accuracy: 0.1744 - val_loss: 0.0457 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0332 - accuracy: 0.1744 - val_loss: 0.0456 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/1500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0331 - accuracy: 0.1744 - val_loss: 0.0454 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0331 - accuracy: 0.1744 - val_loss: 0.0453 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0331 - accuracy: 0.1744 - val_loss: 0.0451 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0331 - accuracy: 0.1744 - val_loss: 0.0450 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0330 - accuracy: 0.1744 - val_loss: 0.0448 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/1500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0330 - accuracy: 0.1744 - val_loss: 0.0446 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/1500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0330 - accuracy: 0.1744 - val_loss: 0.0445 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0329 - accuracy: 0.1744 - val_loss: 0.0443 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0329 - accuracy: 0.1744 - val_loss: 0.0442 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0329 - accuracy: 0.1744 - val_loss: 0.0440 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0328 - accuracy: 0.1744 - val_loss: 0.0439 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/1500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0328 - accuracy: 0.1744 - val_loss: 0.0437 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0328 - accuracy: 0.1744 - val_loss: 0.0435 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0328 - accuracy: 0.1744 - val_loss: 0.0434 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/1500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0327 - accuracy: 0.1744 - val_loss: 0.0432 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0327 - accuracy: 0.1744 - val_loss: 0.0431 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0327 - accuracy: 0.1744 - val_loss: 0.0429 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/1500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0326 - accuracy: 0.1744 - val_loss: 0.0427 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0326 - accuracy: 0.1744 - val_loss: 0.0426 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0326 - accuracy: 0.1744 - val_loss: 0.0424 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0326 - accuracy: 0.1744 - val_loss: 0.0422 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0325 - accuracy: 0.1744 - val_loss: 0.0421 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0325 - accuracy: 0.1744 - val_loss: 0.0419 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0325 - accuracy: 0.1744 - val_loss: 0.0418 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0324 - accuracy: 0.1744 - val_loss: 0.0416 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0324 - accuracy: 0.1744 - val_loss: 0.0414 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0324 - accuracy: 0.1860 - val_loss: 0.0413 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0324 - accuracy: 0.1860 - val_loss: 0.0411 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0323 - accuracy: 0.1977 - val_loss: 0.0410 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0323 - accuracy: 0.1977 - val_loss: 0.0408 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0323 - accuracy: 0.1860 - val_loss: 0.0406 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0323 - accuracy: 0.1860 - val_loss: 0.0405 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0322 - accuracy: 0.1860 - val_loss: 0.0403 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0322 - accuracy: 0.1860 - val_loss: 0.0401 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0322 - accuracy: 0.1860 - val_loss: 0.0400 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0321 - accuracy: 0.1860 - val_loss: 0.0398 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0321 - accuracy: 0.1977 - val_loss: 0.0397 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0321 - accuracy: 0.1977 - val_loss: 0.0395 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0321 - accuracy: 0.1860 - val_loss: 0.0393 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0320 - accuracy: 0.1860 - val_loss: 0.0392 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/1500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0320 - accuracy: 0.1860 - val_loss: 0.0390 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0320 - accuracy: 0.1860 - val_loss: 0.0389 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0320 - accuracy: 0.1860 - val_loss: 0.0387 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0319 - accuracy: 0.1860 - val_loss: 0.0385 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0319 - accuracy: 0.1860 - val_loss: 0.0384 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0319 - accuracy: 0.1860 - val_loss: 0.0382 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0319 - accuracy: 0.1860 - val_loss: 0.0381 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0318 - accuracy: 0.1860 - val_loss: 0.0379 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/1500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0318 - accuracy: 0.1860 - val_loss: 0.0378 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0318 - accuracy: 0.1860 - val_loss: 0.0376 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0318 - accuracy: 0.1860 - val_loss: 0.0374 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0317 - accuracy: 0.1860 - val_loss: 0.0373 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0317 - accuracy: 0.1860 - val_loss: 0.0371 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0317 - accuracy: 0.1860 - val_loss: 0.0370 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0317 - accuracy: 0.1860 - val_loss: 0.0368 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0316 - accuracy: 0.1860 - val_loss: 0.0367 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0316 - accuracy: 0.1860 - val_loss: 0.0365 - val_accuracy: 0.0000e+00\n",
      "Epoch 257/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0316 - accuracy: 0.1860 - val_loss: 0.0364 - val_accuracy: 0.0000e+00\n",
      "Epoch 258/1500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0316 - accuracy: 0.1860 - val_loss: 0.0362 - val_accuracy: 0.0000e+00\n",
      "Epoch 259/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0315 - accuracy: 0.1860 - val_loss: 0.0361 - val_accuracy: 0.0000e+00\n",
      "Epoch 260/1500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0315 - accuracy: 0.1860 - val_loss: 0.0359 - val_accuracy: 0.0000e+00\n",
      "Epoch 261/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0315 - accuracy: 0.1860 - val_loss: 0.0358 - val_accuracy: 0.0000e+00\n",
      "Epoch 262/1500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0315 - accuracy: 0.1860 - val_loss: 0.0356 - val_accuracy: 0.0000e+00\n",
      "Epoch 263/1500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0315 - accuracy: 0.1860 - val_loss: 0.0355 - val_accuracy: 0.0000e+00\n",
      "Epoch 264/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0314 - accuracy: 0.1860 - val_loss: 0.0353 - val_accuracy: 0.0000e+00\n",
      "Epoch 265/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0314 - accuracy: 0.1860 - val_loss: 0.0352 - val_accuracy: 0.0000e+00\n",
      "Epoch 266/1500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0314 - accuracy: 0.1860 - val_loss: 0.0350 - val_accuracy: 0.0000e+00\n",
      "Epoch 267/1500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0314 - accuracy: 0.1860 - val_loss: 0.0349 - val_accuracy: 0.0000e+00\n",
      "Epoch 268/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0313 - accuracy: 0.1860 - val_loss: 0.0347 - val_accuracy: 0.0000e+00\n",
      "Epoch 269/1500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0313 - accuracy: 0.1860 - val_loss: 0.0346 - val_accuracy: 0.0000e+00\n",
      "Epoch 270/1500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0313 - accuracy: 0.1860 - val_loss: 0.0345 - val_accuracy: 0.0000e+00\n",
      "Epoch 271/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0313 - accuracy: 0.1860 - val_loss: 0.0343 - val_accuracy: 0.0000e+00\n",
      "Epoch 272/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0312 - accuracy: 0.1860 - val_loss: 0.0342 - val_accuracy: 0.0000e+00\n",
      "Epoch 273/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0312 - accuracy: 0.1860 - val_loss: 0.0340 - val_accuracy: 0.0000e+00\n",
      "Epoch 274/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0312 - accuracy: 0.1860 - val_loss: 0.0339 - val_accuracy: 0.0000e+00\n",
      "Epoch 275/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0312 - accuracy: 0.1860 - val_loss: 0.0338 - val_accuracy: 0.0000e+00\n",
      "Epoch 276/1500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0312 - accuracy: 0.1860 - val_loss: 0.0336 - val_accuracy: 0.0000e+00\n",
      "Epoch 277/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0311 - accuracy: 0.1860 - val_loss: 0.0335 - val_accuracy: 0.0000e+00\n",
      "Epoch 278/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0311 - accuracy: 0.1860 - val_loss: 0.0334 - val_accuracy: 0.0000e+00\n",
      "Epoch 279/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0311 - accuracy: 0.1860 - val_loss: 0.0332 - val_accuracy: 0.0000e+00\n",
      "Epoch 280/1500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0311 - accuracy: 0.1860 - val_loss: 0.0331 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0310 - accuracy: 0.1860 - val_loss: 0.0330 - val_accuracy: 0.0000e+00\n",
      "Epoch 282/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0310 - accuracy: 0.1860 - val_loss: 0.0328 - val_accuracy: 0.0000e+00\n",
      "Epoch 283/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0310 - accuracy: 0.1860 - val_loss: 0.0327 - val_accuracy: 0.0000e+00\n",
      "Epoch 284/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0310 - accuracy: 0.1860 - val_loss: 0.0326 - val_accuracy: 0.0000e+00\n",
      "Epoch 285/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0310 - accuracy: 0.1860 - val_loss: 0.0324 - val_accuracy: 0.0000e+00\n",
      "Epoch 286/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0309 - accuracy: 0.1860 - val_loss: 0.0323 - val_accuracy: 0.0000e+00\n",
      "Epoch 287/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0309 - accuracy: 0.1860 - val_loss: 0.0322 - val_accuracy: 0.0000e+00\n",
      "Epoch 288/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0309 - accuracy: 0.1860 - val_loss: 0.0321 - val_accuracy: 0.0000e+00\n",
      "Epoch 289/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0309 - accuracy: 0.1860 - val_loss: 0.0319 - val_accuracy: 0.0000e+00\n",
      "Epoch 290/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0308 - accuracy: 0.1860 - val_loss: 0.0318 - val_accuracy: 0.0000e+00\n",
      "Epoch 291/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0308 - accuracy: 0.1860 - val_loss: 0.0317 - val_accuracy: 0.0000e+00\n",
      "Epoch 292/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0308 - accuracy: 0.1860 - val_loss: 0.0316 - val_accuracy: 0.0000e+00\n",
      "Epoch 293/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0308 - accuracy: 0.1860 - val_loss: 0.0314 - val_accuracy: 0.0000e+00\n",
      "Epoch 294/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0308 - accuracy: 0.1860 - val_loss: 0.0313 - val_accuracy: 0.0000e+00\n",
      "Epoch 295/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0307 - accuracy: 0.1860 - val_loss: 0.0312 - val_accuracy: 0.0000e+00\n",
      "Epoch 296/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0307 - accuracy: 0.1744 - val_loss: 0.0311 - val_accuracy: 0.0000e+00\n",
      "Epoch 297/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0307 - accuracy: 0.1744 - val_loss: 0.0310 - val_accuracy: 0.0000e+00\n",
      "Epoch 298/1500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0307 - accuracy: 0.1744 - val_loss: 0.0309 - val_accuracy: 0.0000e+00\n",
      "Epoch 299/1500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0307 - accuracy: 0.1744 - val_loss: 0.0307 - val_accuracy: 0.0000e+00\n",
      "Epoch 300/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0306 - accuracy: 0.1744 - val_loss: 0.0306 - val_accuracy: 0.0000e+00\n",
      "Epoch 301/1500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0306 - accuracy: 0.1744 - val_loss: 0.0305 - val_accuracy: 0.0000e+00\n",
      "Epoch 302/1500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0306 - accuracy: 0.1744 - val_loss: 0.0304 - val_accuracy: 0.0000e+00\n",
      "Epoch 303/1500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0306 - accuracy: 0.1744 - val_loss: 0.0303 - val_accuracy: 0.0000e+00\n",
      "Epoch 304/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0306 - accuracy: 0.1744 - val_loss: 0.0302 - val_accuracy: 0.0000e+00\n",
      "Epoch 305/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0305 - accuracy: 0.1744 - val_loss: 0.0301 - val_accuracy: 0.0000e+00\n",
      "Epoch 306/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0305 - accuracy: 0.1744 - val_loss: 0.0300 - val_accuracy: 0.0000e+00\n",
      "Epoch 307/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0305 - accuracy: 0.1744 - val_loss: 0.0299 - val_accuracy: 0.0000e+00\n",
      "Epoch 308/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0305 - accuracy: 0.1744 - val_loss: 0.0298 - val_accuracy: 0.0000e+00\n",
      "Epoch 309/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0305 - accuracy: 0.1744 - val_loss: 0.0297 - val_accuracy: 0.0000e+00\n",
      "Epoch 310/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0304 - accuracy: 0.1744 - val_loss: 0.0296 - val_accuracy: 0.0000e+00\n",
      "Epoch 311/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0304 - accuracy: 0.1744 - val_loss: 0.0295 - val_accuracy: 0.0000e+00\n",
      "Epoch 312/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0304 - accuracy: 0.1744 - val_loss: 0.0294 - val_accuracy: 0.0000e+00\n",
      "Epoch 313/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0304 - accuracy: 0.1744 - val_loss: 0.0293 - val_accuracy: 0.0000e+00\n",
      "Epoch 314/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0304 - accuracy: 0.1744 - val_loss: 0.0292 - val_accuracy: 0.0000e+00\n",
      "Epoch 315/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0303 - accuracy: 0.1744 - val_loss: 0.0291 - val_accuracy: 0.0000e+00\n",
      "Epoch 316/1500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0303 - accuracy: 0.1744 - val_loss: 0.0290 - val_accuracy: 0.0000e+00\n",
      "Epoch 317/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0303 - accuracy: 0.1744 - val_loss: 0.0289 - val_accuracy: 0.0000e+00\n",
      "Epoch 318/1500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0303 - accuracy: 0.1744 - val_loss: 0.0288 - val_accuracy: 0.0000e+00\n",
      "Epoch 319/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0303 - accuracy: 0.1744 - val_loss: 0.0287 - val_accuracy: 0.0000e+00\n",
      "Epoch 320/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0302 - accuracy: 0.1744 - val_loss: 0.0287 - val_accuracy: 0.0000e+00\n",
      "Epoch 321/1500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0302 - accuracy: 0.1744 - val_loss: 0.0286 - val_accuracy: 0.0000e+00\n",
      "Epoch 322/1500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0302 - accuracy: 0.1744 - val_loss: 0.0285 - val_accuracy: 0.0000e+00\n",
      "Epoch 323/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0302 - accuracy: 0.1744 - val_loss: 0.0284 - val_accuracy: 0.0000e+00\n",
      "Epoch 324/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0302 - accuracy: 0.1744 - val_loss: 0.0283 - val_accuracy: 0.0000e+00\n",
      "Epoch 325/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0301 - accuracy: 0.1744 - val_loss: 0.0282 - val_accuracy: 0.0000e+00\n",
      "Epoch 326/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0301 - accuracy: 0.1744 - val_loss: 0.0282 - val_accuracy: 0.0000e+00\n",
      "Epoch 327/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0301 - accuracy: 0.1744 - val_loss: 0.0281 - val_accuracy: 0.0000e+00\n",
      "Epoch 328/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0301 - accuracy: 0.1744 - val_loss: 0.0280 - val_accuracy: 0.0000e+00\n",
      "Epoch 329/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0301 - accuracy: 0.1744 - val_loss: 0.0279 - val_accuracy: 0.0000e+00\n",
      "Epoch 330/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0301 - accuracy: 0.1744 - val_loss: 0.0279 - val_accuracy: 0.0000e+00\n",
      "Epoch 331/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0300 - accuracy: 0.1744 - val_loss: 0.0278 - val_accuracy: 0.0000e+00\n",
      "Epoch 332/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0300 - accuracy: 0.1744 - val_loss: 0.0277 - val_accuracy: 0.0000e+00\n",
      "Epoch 333/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0300 - accuracy: 0.1744 - val_loss: 0.0277 - val_accuracy: 0.0000e+00\n",
      "Epoch 334/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0300 - accuracy: 0.1744 - val_loss: 0.0276 - val_accuracy: 0.0000e+00\n",
      "Epoch 335/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0300 - accuracy: 0.1744 - val_loss: 0.0275 - val_accuracy: 0.0000e+00\n",
      "Epoch 336/1500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0300 - accuracy: 0.1744 - val_loss: 0.0275 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0299 - accuracy: 0.1744 - val_loss: 0.0274 - val_accuracy: 0.0000e+00\n",
      "Epoch 338/1500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.17 - 0s 96ms/step - loss: 0.0299 - accuracy: 0.1744 - val_loss: 0.0273 - val_accuracy: 0.0000e+00\n",
      "Epoch 339/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0299 - accuracy: 0.1744 - val_loss: 0.0273 - val_accuracy: 0.0000e+00\n",
      "Epoch 340/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0299 - accuracy: 0.1744 - val_loss: 0.0272 - val_accuracy: 0.0000e+00\n",
      "Epoch 341/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0299 - accuracy: 0.1744 - val_loss: 0.0272 - val_accuracy: 0.0000e+00\n",
      "Epoch 342/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0298 - accuracy: 0.1744 - val_loss: 0.0271 - val_accuracy: 0.0000e+00\n",
      "Epoch 343/1500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0298 - accuracy: 0.1744 - val_loss: 0.0270 - val_accuracy: 0.0000e+00\n",
      "Epoch 344/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0298 - accuracy: 0.1744 - val_loss: 0.0270 - val_accuracy: 0.0000e+00\n",
      "Epoch 345/1500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0298 - accuracy: 0.1744 - val_loss: 0.0269 - val_accuracy: 0.0000e+00\n",
      "Epoch 346/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0298 - accuracy: 0.1744 - val_loss: 0.0269 - val_accuracy: 0.0000e+00\n",
      "Epoch 347/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0298 - accuracy: 0.1744 - val_loss: 0.0268 - val_accuracy: 0.0000e+00\n",
      "Epoch 348/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0297 - accuracy: 0.1744 - val_loss: 0.0268 - val_accuracy: 0.0000e+00\n",
      "Epoch 349/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0297 - accuracy: 0.1744 - val_loss: 0.0267 - val_accuracy: 0.0000e+00\n",
      "Epoch 350/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0297 - accuracy: 0.1744 - val_loss: 0.0267 - val_accuracy: 0.0000e+00\n",
      "Epoch 351/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0297 - accuracy: 0.1744 - val_loss: 0.0267 - val_accuracy: 0.0000e+00\n",
      "Epoch 352/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0297 - accuracy: 0.1744 - val_loss: 0.0266 - val_accuracy: 0.0000e+00\n",
      "Epoch 353/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0297 - accuracy: 0.1744 - val_loss: 0.0266 - val_accuracy: 0.0000e+00\n",
      "Epoch 354/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0296 - accuracy: 0.1744 - val_loss: 0.0265 - val_accuracy: 0.0000e+00\n",
      "Epoch 355/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0296 - accuracy: 0.1744 - val_loss: 0.0265 - val_accuracy: 0.0000e+00\n",
      "Epoch 356/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0296 - accuracy: 0.1744 - val_loss: 0.0265 - val_accuracy: 0.0000e+00\n",
      "Epoch 357/1500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0296 - accuracy: 0.1744 - val_loss: 0.0264 - val_accuracy: 0.0000e+00\n",
      "Epoch 358/1500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0296 - accuracy: 0.1744 - val_loss: 0.0264 - val_accuracy: 0.0000e+00\n",
      "Epoch 359/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0296 - accuracy: 0.1744 - val_loss: 0.0264 - val_accuracy: 0.0000e+00\n",
      "Epoch 360/1500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0296 - accuracy: 0.1744 - val_loss: 0.0263 - val_accuracy: 0.0000e+00\n",
      "Epoch 361/1500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0295 - accuracy: 0.1744 - val_loss: 0.0263 - val_accuracy: 0.0000e+00\n",
      "Epoch 362/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0295 - accuracy: 0.1744 - val_loss: 0.0263 - val_accuracy: 0.0000e+00\n",
      "Epoch 363/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0295 - accuracy: 0.1744 - val_loss: 0.0263 - val_accuracy: 0.0000e+00\n",
      "Epoch 364/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0295 - accuracy: 0.1744 - val_loss: 0.0262 - val_accuracy: 0.0000e+00\n",
      "Epoch 365/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0295 - accuracy: 0.1744 - val_loss: 0.0262 - val_accuracy: 0.0000e+00\n",
      "Epoch 366/1500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0295 - accuracy: 0.1744 - val_loss: 0.0262 - val_accuracy: 0.0000e+00\n",
      "Epoch 367/1500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0294 - accuracy: 0.1744 - val_loss: 0.0262 - val_accuracy: 0.0000e+00\n",
      "Epoch 368/1500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0294 - accuracy: 0.1744 - val_loss: 0.0261 - val_accuracy: 0.0000e+00\n",
      "Epoch 369/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0294 - accuracy: 0.1744 - val_loss: 0.0261 - val_accuracy: 0.0000e+00\n",
      "Epoch 370/1500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0294 - accuracy: 0.1744 - val_loss: 0.0261 - val_accuracy: 0.0000e+00\n",
      "Epoch 371/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0294 - accuracy: 0.1744 - val_loss: 0.0261 - val_accuracy: 0.0000e+00\n",
      "Epoch 372/1500\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0294 - accuracy: 0.1744 - val_loss: 0.0261 - val_accuracy: 0.0000e+00\n",
      "Epoch 373/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0294 - accuracy: 0.1744 - val_loss: 0.0261 - val_accuracy: 0.0000e+00\n",
      "Epoch 374/1500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0293 - accuracy: 0.1744 - val_loss: 0.0261 - val_accuracy: 0.0000e+00\n",
      "Epoch 375/1500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0293 - accuracy: 0.1744 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 376/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0293 - accuracy: 0.1744 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 377/1500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0293 - accuracy: 0.1744 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 378/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0293 - accuracy: 0.1744 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 379/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0293 - accuracy: 0.1744 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 380/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0293 - accuracy: 0.1744 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 381/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0292 - accuracy: 0.1628 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 382/1500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0292 - accuracy: 0.1628 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 383/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0292 - accuracy: 0.1628 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 384/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0292 - accuracy: 0.1628 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 385/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0292 - accuracy: 0.1628 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 386/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0292 - accuracy: 0.1628 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 387/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0292 - accuracy: 0.1628 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 388/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0292 - accuracy: 0.1628 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 389/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0291 - accuracy: 0.1628 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 390/1500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0291 - accuracy: 0.1628 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 391/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0291 - accuracy: 0.1628 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 392/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0291 - accuracy: 0.1628 - val_loss: 0.0261 - val_accuracy: 0.0000e+00\n",
      "Epoch 393/1500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0291 - accuracy: 0.1628 - val_loss: 0.0261 - val_accuracy: 0.0000e+00\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 35)                210       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 252       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 56        \n",
      "=================================================================\n",
      "Total params: 518\n",
      "Trainable params: 518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxc9X3v/9dHM6MZ7btlW7ItGWyDwcaAMGYtSwI2uWFpIJjlhvSXX6EltOmjNws0hSbc5oYmbeByQ0rIDSGBEqAQCk1MDcRmN4sMxgveZGNseZVt2dY+0uhz//geWWNZssaWNDM++jwfj3mcmXPOzHx0wO/zne8553tEVTHGGONfGakuwBhjzMiyoDfGGJ+zoDfGGJ+zoDfGGJ+zoDfGGJ8LprqAvkpLS7WqqirVZRhjzHFl6dKlu1W1rL9laRf0VVVV1NbWproMY4w5rojIZwMts64bY4zxOQt6Y4zxOQt6Y4zxubTrozfGmGPR2dlJfX097e3tqS5lREUiESorKwmFQgm/x4LeGOML9fX15OXlUVVVhYikupwRoars2bOH+vp6qqurE36fdd0YY3yhvb2dkpIS34Y8gIhQUlJy1L9aLOiNMb7h55DvcSx/o3+Cvv0ALP4h1C9NdSXGGJNW/BP03V3w+n1Q/36qKzHGjEL79u3jZz/72VG/74orrmDfvn0jUFEv/wR9ON9N2w+ktg5jzKg0UNDHYrEjvm/BggUUFhaOVFmAn866CQQhMxfa96e6EmPMKHTnnXeyYcMGZs2aRSgUIjc3l3HjxrFs2TI++eQTrr76arZs2UJ7ezvf+MY3uPXWW4HeYV+am5uZN28e559/Pu+88w4VFRW88MILZGVlDbk2/wQ9uFZ9hwW9MaPd9/9zFZ9sG95f99PH5/MPXzxlwOX33XcfK1euZNmyZbz22mt84QtfYOXKlQdPg3z00UcpLi6mra2Ns846iy996UuUlJQc8hnr16/nt7/9Lb/4xS/48pe/zHPPPcfNN9885Nr9FfSRAmvRG2PSwuzZsw851/3BBx/k+eefB2DLli2sX7/+sKCvrq5m1qxZAJx55pls2rRpWGrxWdDnWx+9MeaILe9kycnJOfj8tdde49VXX2XJkiVkZ2dz0UUX9XsufDgcPvg8EAjQ1tY2LLX452AsWIveGJMyeXl5NDU19bts//79FBUVkZ2dzZo1a3j33XeTWpu/WvThfNhTl+oqjDGjUElJCeeddx6nnnoqWVlZlJeXH1w2d+5cHn74YWbOnMm0adOYM2dOUmvzV9Bbi94Yk0JPPvlkv/PD4TAvvfRSv8t6+uFLS0tZuXLlwfnf/OY3h60un3XdeH30qqmuxBhj0obPgr4Aujuhc3gOYBhjjB/4K+h7ro7tsDNvjDGmh7+CPlLgptZPb4wxByUU9CIyV0TWikidiNzZz/ILReRDEekSkWv7LJsoIi+LyGoR+UREqoan9H4cDHpr0RtjTI9Bg15EAsBDwDxgOnCDiEzvs9pm4KtAf4ecfwP8WFVPBmYDu4ZS8BFZi94YYw6TSIt+NlCnqhtVNQo8BVwVv4KqblLV5UB3/HxvhxBU1Ve89ZpVtXV4Su/HwT56C3pjTHId6zDFAA888ACtrSMXjYkEfQWwJe51vTcvEVOBfSLyOxH5SER+7P1COISI3CoitSJS29DQkOBH98Na9MaYFEnnoE/kgqn+7luV6InqQeAC4HRc987TuC6eXx7yYaqPAI8A1NTUHPtJ8BEbk94YkxrxwxR//vOfZ8yYMTzzzDN0dHRwzTXX8P3vf5+Wlha+/OUvU19fTywW4+6772bnzp1s27aNiy++mNLSUhYvXjzstSUS9PXAhLjXlcC2BD+/HvhIVTcCiMh/AHPoE/TDJpQNErAWvTGj3Ut3wo4Vw/uZY2fAvPsGXBw/TPHLL7/Ms88+y/vvv4+qcuWVV/LGG2/Q0NDA+PHj+cMf/gC4MXAKCgr4yU9+wuLFiyktLR3emj2JdN18AEwRkWoRyQTmAy8m+PkfAEUiUua9vgT45OjLTJCI676x8+iNMSn08ssv8/LLL3P66adzxhlnsGbNGtavX8+MGTN49dVX+c53vsObb75JQUFBUuoZtEWvql0icgewEAgAj6rqKhG5F6hV1RdF5CzgeaAI+KKIfF9VT1HVmIh8E/ijuFuXLwV+MXJ/Dt4wCNaiN2ZUO0LLOxlUlbvuuovbbrvtsGVLly5lwYIF3HXXXVx22WXcc889I15PQoOaqeoCYEGfeffEPf8A16XT33tfAWYOocajEymwPnpjTNLFD1N8+eWXc/fdd3PTTTeRm5vL1q1bCYVCdHV1UVxczM0330xubi6PPfbYIe8dqa4bf41eCd7tBC3ojTHJFT9M8bx587jxxhs555xzAMjNzeWJJ56grq6Ob33rW2RkZBAKhfjXf/1XAG699VbmzZvHuHHjRuRgrGiajfRYU1OjtbW1x/4BT90EezfC7UuGryhjTNpbvXo1J598cqrLSIr+/lYRWaqqNf2t76+xbsC6bowxpg+fBr0djDXGmB4+DPpCiDZBrDPVlRhjkizduqJHwrH8jf4L+uxiN21rTG0dxpikikQi7Nmzx9dhr6rs2bOHSCRyVO/z31k32SVu2roHcsekthZjTNJUVlZSX1/PkMbLOg5EIhEqK/s9m31APgx6r0Xfuie1dRhjkioUClFdXZ3qMtKSD7tuelr0e1NbhzHGpAn/BX2WteiNMSae/4L+4MFYa9EbYwz4MehDWRDKsa4bY4zx+C/owbXqLeiNMQbwddBbH70xxoBfgz7Lgt4YY3r4M+izS+xgrDHGePwb9NaiN8YYwLdBX+xGsIx1pboSY4xJuYSCXkTmishaEakTkTv7WX6hiHwoIl0icm0/y/NFZKuI/HQ4ih5Uz9WxNrCZMcYMHvQiEgAeAuYB04EbRGR6n9U2A18FnhzgY/4n8Pqxl5kYVaUr1g1ZRW6G9dMbY0xCLfrZQJ2qblTVKPAUcFX8Cqq6SVWXA9193ywiZwLlwMvDUO+Adh1oZ8p3X+KZ2vpDR7A0xphRLpGgrwC2xL2u9+YNSkQygH8BvjXIereKSK2I1B7rEKP5WSG6upXG1qgFvTHGxEkk6KWfeYmO7H87sEBVtxxpJVV9RFVrVLWmrKwswY8+VCQUICczwN6WaNxQxdZ1Y4wxiYxHXw9MiHtdCWxL8PPPAS4QkduBXCBTRJpV9bADusOhKCeTxpaojWBpjDFxEgn6D4ApIlINbAXmAzcm8uGqelPPcxH5KlAzUiEPUJyTyd7WKGRmu4HNWnaP1FcZY8xxY9CuG1XtAu4AFgKrgWdUdZWI3CsiVwKIyFkiUg9cB/xcRFaNZNEDKcz2WvQAeeXQvCMVZRhjTFpJ6FaCqroAWNBn3j1xzz/Adekc6TMeAx476gqPQnF2iE93N7sXeeOgyYLeGGN8dWWs66PvdC/yxlrQG2MMPgv64uxMmju6iHZ1Q64X9JroCULGGONPvgr6opxMAPa1Rl2LvrMFOppSXJUxxqSWr4K+2Av6va1R10cP0LwzhRUZY0zq+Sroi7K9oG+JurNuAJq2p7AiY4xJPV8FfU+LvrGls7dFbwdkjTGjnK+Cvig7BPR03Yx1My3ojTGjnK+CvjC7p0UfhXCeuzrWgt4YM8r5KugzgxnkhYOujx68c+mtj94YM7r5KujBu2iqtSfo7epYY4zxZdDvtfFujDHmIN8FfXF2iH2tPcMgjLOrY40xo57vgv7QFv1Y6GyF9v2pLcoYY1LId0FfnB0X9IUT3XTf5tQVZIwxKea/oM/NpK0zRmu0C4qq3MzGT1NakzHGpJLvgr40NwzAnuZob9DvtaA3xoxePgx6d9HU7uYOiBS4+8c2bkptUcYYk0I+DHrXot/d7PXTF1db140xZlRLKOhFZK6IrBWROhE57ObeInKhiHwoIl0icm3c/FkiskREVonIchG5fjiL70/Jwa6bDjejqNq6bowxo9qgQS8iAeAhYB4wHbhBRKb3WW0z8FXgyT7zW4GvqOopwFzgAREpHGrRR1KSE9d1A66ffn89xDpH8muNMSZtJdKinw3UqepGVY0CTwFXxa+gqptUdTnQ3Wf+OlVd7z3fBuwCyoal8gFEQgHywsFDu240Bvu3jOTXGmNM2kok6CuA+JSs9+YdFRGZDWQCG/pZdquI1IpIbUNDw9F+9GFK88KHtujBum+MMaNWIkEv/cw7qjEFRGQc8DjwZ6ra3Xe5qj6iqjWqWlNWNvQGf2luZlzQV7upnXljjBmlEgn6emBC3OtKYFuiXyAi+cAfgL9X1XePrrxjU5ITdufRgxvvJhC2M2+MMaNWIkH/ATBFRKpFJBOYD7yYyId76z8P/EZV//3Yyzw6pXlxLfqMDNdPv7suWV9vjDFpZdCgV9Uu4A5gIbAaeEZVV4nIvSJyJYCInCUi9cB1wM9FZJX39i8DFwJfFZFl3mPWiPwlcUpywjS2dtIV83qJxpwMDatH+muNMSYtBRNZSVUXAAv6zLsn7vkHuC6dvu97AnhiiDUetdI8dy793pYoY/IjMGY6rHoeOpohnJvscowxJqV8d2UsQOnBc+m9fvoxJ7tpw9oUVWSMManjz6DP6xkGweunH+Nd37Vr1QDvMMYY//Jl0PdcHbunJe4Uy8w82P5xCqsyxpjU8GXQH2zRN3ldNxkZMH4WbPsohVUZY0xq+DLo88JBMoMZvV034IJ+x0ob88YYM+r4MuhFhPL8MLua4oP+dIh1wM6VqSvMGGNSwJdBD1CeF2HngfbeGRPPddNNb6WmIGOMSRHfBv2Y/PChQZ8/DkqmwKdvpq4oY4xJAf8GfV6EXQc6Dp1ZfQF89g7EulJTlDHGpIBvg748P0JTRxctHXGhXn0hRJtg+7LUFWaMMUnm46B3p1geckC26gI3/fT1FFRkjDGp4eOgjwAc2k+fUwpjToENi1NUlTHGJJ+Pg9616A8JeoCTvgCfvQ3Nu1JQlTHGJJ9vg36M16I/7IDsqX8K2g2fvJCCqowxJvl8G/R54SBZocDhLfoxJ7tBzlY+l5rCjDEmyXwb9D1Xx+5s6jh84al/CpuXwP765BdmjDFJ5tugB3cu/WEteoBTrwUEPnw86TUZY0yy+Tvo88M09NeiL66GKZdB7aPQ1c9yY4zxkYSCXkTmishaEakTkTv7WX6hiHwoIl0icm2fZbeIyHrvcctwFZ6I8nzXolfVwxeefRu07HK3GDTGGB8bNOhFJAA8BMwDpgM3iMj0PqttBr4KPNnnvcXAPwBnA7OBfxCRoqGXnZjy/DCt0RhNHf0MeXDCJVA6FZY8BP3tCIwxxicSadHPBupUdaOqRoGngKviV1DVTaq6HOju897LgVdUda+qNgKvAHOHoe6EjCvIAmD7vn766UXg3L+GHcth3cJklWSMMUmXSNBXAFviXtd78xKR0HtF5FYRqRWR2oaGhgQ/enDjC13Qb9vX1v8Kp82Hwknw2g+tVW+M8a1Egl76mZdoKib0XlV9RFVrVLWmrKwswY8eXIUX9FsHCvpACC78lhvkzFr1xhifSiTo64EJca8rgW0Jfv5Q3jtkZXlhghkycIseXKu+qMpa9cYY30ok6D8ApohItYhkAvOBFxP8/IXAZSJS5B2EvcyblxSBDGFcYWTgFj0c2qpf+1KySjPGmKQZNOhVtQu4AxfQq4FnVHWViNwrIlcCiMhZIlIPXAf8XERWee/dC/xP3M7iA+Beb17SjC/IOnKLHmDmfCiqhsX/C7r7Hk82xpjjWzCRlVR1AbCgz7x74p5/gOuW6e+9jwKPDqHGIakozOK9TwfZtwSCcNGd8PxtsPpFOOXq5BRnjDFJ4OsrY8GdebPjQDtdsUFa6jOuc+fVv/ZD6I4lpzhjjEkC3wd9RVEWsW7tf3CzeBkB16pvWGNXyxpjfMX3QT/oufTxpl/j7kD12g/tBuLGGN/wfdBXFLobkCQU9BkZcPFdsKcOVvz7CFdmjDHJ4fug7xkG4YinWMY76b/B2Jnw+n0Q6xzByowxJjl8H/Q54SCF2SG2NiYY9CJw8XehcRMse3LQ1Y0xJt35PugBJhRls3lva+JvmHo5VNTAGz+GrujIFWaMMUkwKoJ+UslRBr0IXPx3sH8LfPSbkSvMGGOSYFQEfVVJDvWNbXQOdi59vBMugQlz4I1/gc5+hjk2xpjjxKgI+kkl2cS6NfF+enCt+ku+C03bYOljI1abMcaMtFER9FWlOQBs2tNydG+svhCqLoC3fgLRo+j6McaYNDIqgn5SSTYAn+05hrC++O+geSfU/nKYqzLGmOQYFUFflhsmOzNw9C16gEnnwuSL4a37oX3/8BdnjDEjbFQEvYgwsTj72Fr0AJ/7HrTuhdd/NJxlGWNMUoyKoAd35s0xtegBxs+C02+C934OezYMb2HGGDPCRk3QTyrNpn5vG7HuY7xd4CX3QDACL//98BZmjDEjbNQEfVVJDtFYd2KDm/Unrxwu/B+wdgGs/a/hLc4YY0bQqAn6yd4plhsamo/9Q+bcDuWnwn/+teuzN8aY40BCQS8ic0VkrYjUicid/SwPi8jT3vL3RKTKmx8SkV+LyAoRWS0idw1v+YmbWp4HwPqdQwj6YBiuediF/H9+A/QYu4GMMSaJBg16EQkADwHzgOnADSIyvc9qXwMaVfVE4H7gn7z51wFhVZ0BnAnc1rMTSLainExKc8Os29k0tA8aOwMuvcfdW/aNHw9PccYYM4ISadHPBupUdaOqRoGngKv6rHMV8Gvv+bPApSIigAI5IhIEsoAocGBYKj8GU8tzhx70AOf+FcycD4t/AEt/Pfj6xhiTQokEfQWwJe51vTev33VUtQvYD5TgQr8F2A5sBv5ZVVPWuT21PI/1u5rpPtYzb3qIwJUPwomfc/31b/wYuo9iwDRjjEmiRIJe+pnXNykHWmc2EAPGA9XA/xCRyYd9gcitIlIrIrUNDQ0JlHRsppbn0RqNJX63qSMJhmH+b2HGdbDoH+GxK2Dr0qF/rjHGDLNEgr4emBD3uhLYNtA6XjdNAbAXuBH4L1XtVNVdwNtATd8vUNVHVLVGVWvKysqO/q9I0NTyXADW7xqG7huAYCb86S/g6odh12r4xSXwyEWw+H9B3R+hZffwfI8xxgxBMIF1PgCmiEg1sBWYjwvweC8CtwBLgGuBRaqqIrIZuEREngCygTnAA8NV/NGa4p15s25nM5ecVD48HyoCs26Ak77gbj244hnXlaNeV07eeBg3E8adBpVnweSLIBAanu82xpgEDBr0qtolIncAC4EA8KiqrhKRe4FaVX0R+CXwuIjU4Vry8723PwT8CliJ6975laouH4G/IyEFWSHK84fhzJv+RPJhzl+4R9s+2P4x7FgO25e76fqXXfjnlMHsW905+eHc4a/DGGP6EE2zc8Framq0trZ2xD7/lkffZ+eBdv7rby4cse/oV7QVPn0dan8F6xdCbjl88UGYNje5dRhjfElElqrqYV3jMIqujO0xs7KA9buaaYvGkvvFmdkwbR7c9Ax87RXXsv/t9fDSd6A7ybUYY0aVURf0MyoKiHUrn2xP4djyE2bDny+Cs/8S3nsYnroJosc4sqYxxgxi1AX9aRMKAVhen+KbiATDMO8+uOKfXVfOY1+w8XOMMSNi1AV9eX6E8vxw6oO+x+w/h/lPws5V8PjV0NaY6oqMMT4z6oIeYEZFIcvr96W6jF7T5sH1/+bOxf/N1e6sHWOMGSajMuhPqyxg4+4Wmto7U11Kr6mXwfVPeC37ayzsjTHDZlQG/YzKAlRhRbp03/SYejlc/zjsWOHC3vrsjTHDYFQG/ekTi8gQeO/TNAzSafNc2O9cCb++0oZRMMYM2agM+oKsEKeML+DdjXtSXUr/ps2DG56CPXXwqyvgwPZUV2SMOY6NyqAHOOeEEj7avI/2zjS9WOnES+HmZ2F/PfxqLjSsTXVFxpjj1KgN+jmTi4nGuvlwcxqfzlh1PtzyoruY6heXwrqFqa7IGHMcGrVBX1NVTIbAuxvStPumR2UN/PliKK6GJ6+HV78HXR2prsoYcxwZtUGfHwkxo6KAd9I96AEKJ8D/txBOvxneut+Ne79tWaqrMsYcJ0Zt0ANcMKWMDzc30tgSTXUpg8vMhqt+Cjc8DS0N7gYnL3wdmnamujJjTJob1UH/+enldCssWrMr1aUkbtpc+Pr7cM7X4eOn4f+cAa//CDpGYIx9Y4wvjOqgn1FRwNj8CK98cpy1irMK4fIfwNffc3esWvwDeGAmvP2gG/feGGPijOqgz8gQPjd9DK+va0jf0yyPpOQEmP9v8P8vgvGz4JW74cFZ8N7P7YCtMeagUR30AJ+fPpa2zhhvrT+Or0CtPBP++/PwZ/8FJVPgpW/Dg6e7u1nF0mg8H2NMSoz6oD9ncglF2SGe/2hrqksZuknnwFd/D195AfLHw+//Bn5a425aHutKdXXGmBRJKOhFZK6IrBWROhG5s5/lYRF52lv+nohUxS2bKSJLRGSViKwQkcjwlT90mcEMrppVwSuf7GRf63Fw9s1gRFy//ddegRv/HcL58B9/CT+bAyuehe7uVFdojEmyQYNeRALAQ8A8YDpwg4hM77Pa14BGVT0RuB/4J++9QeAJ4C9U9RTgIiDt+hKuq6kkGuvmxY+3pbqU4SPihj6+7Q03/HFGEJ77Gjx8Pqz+PaTZTeGNMSMnkRb9bKBOVTeqahR4CriqzzpXAb/2nj8LXCoiAlwGLFfVjwFUdY+qpt1Rz1PGF3DyuHyeqd2C+i0AReDkL8Jfvg1f+iXEOuDpm+DnF8BHT0BnW6orNMaMsESCvgLYEve63pvX7zqq2gXsB0qAqYCKyEIR+VBEvt3fF4jIrSJSKyK1DQ0NR/s3DIsbZ09g5dYDLP0sjce+GYqMAMy4Fm5/D676meuzf+Hr8JOTYeF3YdtH1so3xqcSCXrpZ17fRBhonSBwPnCTN71GRC49bEXVR1S1RlVrysrKEihp+H3pzEoKs0M88sbGlHx/0gSCcPpNcPsSuOX3UH0hvPewu9L2wdPdWDqfvWNn6xjjI8EE1qkHJsS9rgT6dmb3rFPv9csXAHu9+a+r6m4AEVkAnAH8cYh1D7vszCBfmTOJ/7O4jo0NzUwuy011SSNLBKovcI/WvbDm97DqeXfR1Vv3Q2YuVF0AJ1wMk86DMdMhY9SfpGXMcSmRoP8AmCIi1cBWYD5wY591XgRuAZYA1wKLVFVFZCHwbRHJBqLAn+AO1qal/35OFT9/YyM/XVTHT66flepykie7GM74inu07YNNb8KGRVD3R1j3klsnUgiTzu19jD3N/TowxqS9Qf+lqmqXiNwBLAQCwKOqukpE7gVqVfVF4JfA4yJSh2vJz/fe2ygiP8HtLBRYoKp/GKG/ZcjK8sJ89dwqHnlzI39+4WROHpef6pKSL6vQHbw9+YvudeNnrivns7fddO0CNz+UAxPP9oL/PBh/BoTS6sxZY4xH0u0sk5qaGq2trU3Z9+9rjXLhjxZzxqQiHvuz2SmrI2017fCC33vsWuXmB8Ju7PyeFn/lbAj7vPvLmDQiIktVtabfZRb0h3v49Q3c99IafnlLDZeeXJ7SWtJe617Y/G5vi3/7x6AxkAzXr19xptsBVNRA2TR39o8xZthZ0B+laFc3Vzz4Jm3RGK/87YVkZ1pfdMI6mmDL+7DlPaivha1LoX2fW5aZC+NPPzT888eltl5jfMKC/hh8sGkv1z28hD87r4p/+OIpqS7n+KUKezbA1lov+Gthx0ro9k7fzBsPFWe48K840+0IIqPw2IgxQ3SkoLem6gDOqirmK+dM4ldvb+JPppZx0bQxqS7p+CQCpSe6x2nz3bzOdtixwrX2ex5rft/zBiid6gW/twMoPxWCmSn7E4w53lmL/gjaO2Nc+dO32NvSyYK/Pp8x+XZWyYhp3euuzt36oRf+te6WiQCBTBg7s7fVX3EmFE+28/qNiWNdN0OwdkcT1/zsbaaMyeXp284hErKDiUmhCvu3xLX6P3Q7gk7vDlqRAndKZ3z459mBczN6WdAP0cJVO7jt8aVcedp4/vf8Wbjx2kzSxbpg99pDu3x2fuLO8gHIr4QJs2HiOW5s/jHT7SwfM2pYH/0QXX7KWL51+TR+vHAtE4qz+OZl0yzsUyEQhPJT3OOMr7h50VbYsdyFfn2tO9Vz1e/csnCBF/xz3Ln9dlGXGaUs6BN0+0UnUN/YykOLNxAKZPA3n5ua6pIMQGa2C/KJc9xrVdi32QX+5nfcdNErblkg04X9xDm9rf5IQepqNyZJLOgTJCL84OoZdMaUB15dD8A3Lp1iLft0IwJFk9zjtOvdvJ6LujYvcY8lD8HbD7iLusbNciN4Vl/odgCZOamt35gRYH30RynWrXz72eU892E919dM4B+vOZVQwM7+OK5EW91ZPZvegk/fgPoPoLsLMkJQeZY3queF7nkwnOpqjUmIHYwdZqrK/a+s48FFdZx3YgkPzj+dklwLhONWtMW19D990wX/9mWg3RCMwISzXeifcAmMO80O7pq0ZUE/Qp5dWs/f/W4Fhdkh7r9+FuedWJrqksxwaNvnxu359A03ZPPOlW5+VrG78foJl7hx+gsqU1mlMYewoB9Bn2w7wF/99kM27m7hlnOq+NvLppIfCaW6LDOcmhtg42tujP4Ni6B5h5tfOs0L/Uug6jzr3zcpZUE/wlqjXdz30hoef/czSnLC3DnvJK6eNZ6g9d37jyrsWt0b+p+9DV3t7oyeCWf3Bv/YmXblrkkqC/okWVG/n79/YSUfb9nH5NIc/urSE/nCjPFkBu0fvG91trv+/Q2LYMNi2LnCzc8ugckX9wa/jdJpRpgFfRJ1dysvf7KDB15dz5odTZTlhblh9kSuPaOSiSXZqS7PjLSmnYd287TscvPHnAInXgInXOou3rKzecwws6BPge5u5fV1DfxmySZeW9eAKsyoKOCKGeP4/PQxnFCWa+fg+52qO5Bb90fY8Ed3Ln8sCsEsqDofTrzUBX/pFHf+vzFDMOSgF5G5wP/G3TP2/6rqfX2Wh4HfAGcCe4DrVXVT3PKJwCfA91T1n4/0XX4J+nj1jWXS558AABB3SURBVK0sWLGdP6zYwcdb3E04yvPDnHdCKeedWEpNVRETi7Mt+P0u2uLO3a971YX/3g1ufsHE3tb+5D+xq3XNMRlS0ItIAFgHfB6ox93o+wZV/SRunduBmar6FyIyH7hGVa+PW/4c0A28NxqDPl59Yytvrd/NW3W7WbJhD3taogAU52RyWmUBsyYUcdqEAk6rLKQox8Zg97XGTV5rfxFsfB2iTSABd6FWT2t//Cw7d98kZKhBfw6uJX659/ouAFX9Ydw6C711lohIENgBlKmqisjVwHlAC9A82oM+Xne3snZnEx9t3seyLY0s27KP9bua6flPUp4f5qSx+Zw0Lo+Tvenk0lw7uOtHsU53hW5PN8+2ZYBCVpE7qNsT/HZQ1wxgqKNXVgBb4l7XA2cPtI6qdonIfqBERNqA7+B+DXzzCAXeCtwKMHHixARK8oeMDOHkcfmcPC6fG892f3dTeycr6vezctt+1mxvYvWOJt7ZsJvOmEv/UECYXJpLVWk21aW5TC7Noao0h+rSHEpzM63753gVCLmDtJPOhUvvhpY9sHFxb/D3jMg5Zro7i+fES2HiuTYap0lIIkHfX3L0/Rkw0DrfB+5X1eYjBZCqPgI8Aq5Fn0BNvpUXCXHuiaWcG3eVbWesm40NLazZcYA1O5pYv7OJul3NLFqz6+AOACAvHGRiSTaVRVlUFGZTUZTlPXfTgqyQ7QiOFzklMONa91CFnatc4Nf9Ed5/BJb81Duoe55r6Z94qbsFo/33Nf1IJOjrgQlxryuBbQOsU+913RQAe3Et/2tF5EdAIdAtIu2q+tMhVz6KhAIZTBubx7SxeVwVN78r1s22fe1s3N3Mp7tb2LS7hU17WtnQ0MIb63bT1hk75HNyw0EqCrOoKMpifGGEsfkRyr3H2III5XkR8rOCtjNINyIw9lT3OO8b3kHdt73gfxUW3gULgYIJbmiGEz8H1X8CWYWprtykiUT66IO4g7GXAltxB2NvVNVVcet8HZgRdzD2T1X1y30+53tYH33SqCqNrZ3UN7aytbGNrfvaqG90j6372ti2r439bZ2HvS8SynDhnxehvCBCeV6YsQURxuRHKM3NpCw3TElumMKsEBkZtkNIC42f9bb2P30DOg54B3Vrelv740+3g7o+NxynV14BPIA7vfJRVf2BiNwL1KrqiyISAR4HTse15Oer6sY+n/E9LOjTSntnjJ0H2tl5oIMdB9rZdaCdHfvb2dnU4c13rzu6ug97byBDKM7JpCQnk7K8MCU5mZR6O4HS3J7nvdNw0EImKWKd7k5bPcG/7SN6D+pe5A7sVl8ARdXWzeMzdsGUOWaqyoG2LnY2tbO7qYPdLVF2N3Wwp6WDPc1Rdjd3sPvgtIP2zsN3CuCOHxTmhCjKzqQwO5OibPe8KDuTopwQhdmZFGdnUpgdoijHLc8KBawbaah6DupuWOSCv2dAtvxKd9FW9QVQdYG7UYs5rlnQm6RpjXaxuynK7pYOb4cQZY+3M9jXGqWxtfPgtLElSlNH14CflRnM6A1/b4dQkJVJQVaI/Kygm0ZC3usQ+ZHgwed2M5h+qMLudW7o5U/fdBdvte52ywom9oZ+9QU2BPNxyILepK3OWDf74sJ/b0vfHUKUvS29z/e3dXGgrZNorP9fDj2yMwPkRwbeIeR7zwuyQuRFguSFQ+RGguSGg+RFgoSDGf7/NaEKDWu80H/DBX9bo1tWVOVCf9K5blTO4snW1ZPmLOiNr6gqHV3d7G/r5EBbp5u2d3KgrWvgee2dB5c1dXQx2P/2wQw5GPw94Z8bDpIbCR36OhwkNxIkz5v2LnM7juxQ4Pg5aN3dDbs+6W3xf/YWtO93y3LGwMSzYYJ3Y/VxM925/yZtWNAbE6e7W2nq6Dq4Q2ju6KKpvYvmjk6a27to6uiiub2LZm96yOu4dQc6HhFPBHIyg2RnBsgJe9PMINnhwOHzE1yenRkkkIydR3e3a/FveRc2v+eGY973mVsWzIKKM90N1SfOcc+zi0e+JjMgC3pjRkBnrJuWg8Hf346hd8fRFo3REo3R2tFFS7SL1miMlo7eaUs0Rqw78X+LkVDGwDuEzCCRzADZoQBZmQEioQBZ3vPsPq+zQu51dmbvvCN2WzXtcKNwbn7X7QC2Lwf1rtcoqnaBX3GGm46dCZk2NHeyDHUIBGNMP0KBDAq9s4iGSlWJxrpp7Yj1uyNojXrzj7C8uaOLXQc6aIl20d4Zoy0ao7UzNmg3VV8iEAnG7RR6dgIHn08gK7OKSOlN5I/pYHJ0DRPbVjO2eTVj6t4kZ+Wz7m+SAK2FU2grm0Vn+Sxi42aRMeYkwpFsIt4OJim/TIy16I3xs57jGe2dMdp6wj8aO+R1W6d73drz3Ju2dcate3BeN+3RGK2dXbRFu733dRH/Y6SMRk7L2MjMjA3Mkg3MzNhIobQA0KUZbNDxrNaJrO6exDomsTE4meZgMZFQgHAog3AwQCSUQTiY4eZ500jQLY+fFw5mEA4FiPQz7fm8nvdlBjIIBd3UjwfbrUVvzCglIgdbzyM1IIKq0hnTuJ1B746hoyvGR9EYGfs+JWvPKnIaV5O/fy2XNq3j6o53Dn5GsxSxNXACW4InsCUwgc1Usqmrgj3RbDo6u2nvcjucnp1WIsdHBhMKCJmBDDKDcY9ABiFvRxA/zz0P9D4PSNzywCGfEfbWCQX6+YxABqGguGWBDIIB97zndSggI3KvaQt6Y8yQiAiZQRd8BQx0Jk45MOfQWa173WBtO1eSu2Ml03auYNqu/4BYR+86OWPcYG1lU920dCqUTEXzK4hqBu2d3XR0xejwpu1x0/gdQ0dXN9GeR6zPtM/8jq5uOr3n7Z3dHGjrOmx5tCtGZ8x1tx3NsZXBnDahkBe+ft6wfV4PC3pjTGpkF7uLs6ov6J3XHXNn9jSscxd37V4Lu9fDyt9B+76Dq4kECBdUEi6qcuf8H3xMgtIqd3P2JHXNxLr14M6iIxY7ZMfR2aVEY707ms6Y0hXzlsWUzli393DPS0boZkMW9MaY9JERcBdnFU+GaXN756tCy24X/Hs/dXfn6nms+UPvFb49ghHIGwt5493NWvLGQf743mnuGLczCOcPeYcQyBB3kDozAAP+okktC3pjTPoTgdwy96g6//DlHU1uFM/GTe4XwYFt0LTdnQ669UP3vKv98PdlBF3gZ5dAVrH7lZFd4qbhPMjMjZvmQmaeN811p44GMiEQdjuoND64a0FvjDn+hfN6x+zvj6ob3qFpOxzYDi0N0Lrn0Edbo+suat3jjh9orP/P6pe40A+G3RXDgUw3lUEOrKq6R3eXe4ybCTc/dxTfmxgLemOM/4l4rfViKD9l8PVVobMNos3u10JHk/e8uXdeZxvEom5o6FjUHUTued7lPT/sZnz91RZwvwgyglBcPeQ/tT8W9MYY05eI65rJzHb9+cc5G8vVGGN8zoLeGGN8zoLeGGN8LqGgF5G5IrJWROpE5M5+lodF5Glv+XsiUuXN/7yILBWRFd70kuEt3xhjzGAGDXoRCQAPAfOA6cANIjK9z2pfAxpV9UTgfuCfvPm7gS+q6gzgFtwNxI0xxiRRIi362UCdqm5U1SjwFHBVn3WuAn7tPX8WuFRERFU/UtVt3vxVQEREwsNRuDHGmMQkEvQVwJa41/XevH7XUdUuYD9Q0medLwEfqWpHn/mIyK0iUisitQ0NDYnWbowxJgGJBH1/1/X2vQrgiOuIyCm47pzb+vsCVX1EVWtUtaasrCyBkowxxiQqkQum6oEJca8rgW0DrFMvIkGgANgLICKVwPPAV1R1w2BftnTp0t0i8lkCdQ2kFHdsIN2ka11gtR0rq+3YpGtt6VoXJFbbpIEWJBL0HwBTRKQa2ArMB27ss86LuIOtS4BrgUWqqiJSCPwBuEtV307gu1DVITXpRaR2oLuspFK61gVW27Gy2o5NutaWrnXB0GsbtOvG63O/A1gIrAaeUdVVInKviFzprfZLoERE6oC/BXpOwbwDOBG4W0SWeY/j/3piY4w5jiQ01o2qLgAW9Jl3T9zzduC6ft73j8A/DrFGY4wxQ+DHK2MfSXUBA0jXusBqO1ZW27FJ19rStS4YYm2iOnz3OzTGGJN+/NiiN8YYE8eC3hhjfM43QT/YwGspqGeTN5jbMhGp9eYVi8grIrLemxYlqZZHRWSXiKyMm9dvLeI86G3H5SJyRgpq+56IbI07U+uKuGV3ebWtFZHLR7CuCSKyWERWi8gqEfmGNz/l2+0ItaXDdouIyPsi8rFX2/e9+dXegIfrvQEQM735/Q6ImOTaHhORT+O22yxvfrL/LQRE5CMR+b33evi2maoe9w8gAGwAJgOZwMfA9BTXtAko7TPvR8Cd3vM7gX9KUi0XAmcAKwerBbgCeAl3tfMc4L0U1PY94Jv9rDvd+28bBqq9/+aBEaprHHCG9zwPWOd9f8q32xFqS4ftJkCu9zwEvOdtj2eA+d78h4G/9J7fDjzsPZ8PPD2C222g2h4Dru1n/WT/W/hb4Eng997rYdtmfmnRJzLwWjqIH/zt18DVyfhSVX0D70rlBGq5CviNOu8ChSIyLsm1DeQq4ClV7VDVT4E63H/7kahru6p+6D1vwl1DUkEabLcj1DaQZG43VdVm72XIeyhwCW7AQzh8ux02IGKSaxtI0v6bihtB4AvA//VeC8O4zfwS9IkMvJZsCrwsbhz+W7155aq6Hdw/ViCVF48NVEu6bMs7vJ/Lj8Z1caWkNu+n8em4FmBabbc+tUEabDevC2IZsAt4BfcLYp+6iy/7fn8iAyKOWG2q2rPdfuBtt/uld4TdZG63B4BvA93e6xKGcZv5JegTGXgt2c5T1TNw4/h/XUQuTHE9iUqHbfmvwAnALGA78C/e/KTXJiK5wHPA36jqgSOt2s+8ZNeWFttNVWOqOgs3LtZs4OQjfH9KaxORU4G7gJOAs4Bi4DvJrE1E/huwS1WXxs8+wncfdV1+CfpEBl5LKvXG4VfVXbhB3WYDO3t++nnTXamrcMBaUr4tVXWn9w+yG/gFvd0MSa1NREK4IP03Vf2dNzsttlt/taXLduuhqvuA13D924XiBjzs+/0Ha5M+AyImqba5XleYqhtC/Vckf7udB1wpIptw3c6X4Fr4w7bN/BL0Bwde845Mz8cNtJYSIpIjInk9z4HLgJX0Dv6GN30hNRXCEWp5EfiKd8bBHGB/T1dFsvTpB70Gt+16apvvnXVQDUwB3h+hGgQ3htNqVf1J3KKUb7eBakuT7VYmbjBDRCQL+BzuGMJi3ICHcPh269meBwdETGJta+J23ILrB4/fbiP+31RV71LVSlWtwmXXIlW9ieHcZiN5FDmZD9wR8nW4/sDvpriWybizHD7G3Vnru978EuCPwHpvWpyken6L+ynfiWsNfG2gWnA/Cx/ytuMKoCYFtT3uffdy73/qcXHrf9erbS0wbwTrOh/3c3g5sMx7XJEO2+0ItaXDdpsJfOTVsBK4J+7fxPu4A8H/DoS9+RHvdZ23fHIKalvkbbeVwBP0npmT1H8L3ndeRO9ZN8O2zWwIBGOM8Tm/dN0YY4wZgAW9Mcb4nAW9Mcb4nAW9Mcb4nAW9Mcb4nAW9Mcb4nAW9Mcb43P8DQTqswPDa61EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras import  initializers\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import stats \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "model = keras.Sequential()  \n",
    "#model.add(Dropout(0.65))\n",
    "#model.add(Dense(5, input_dim=11, activation='relu'))\n",
    "model.add(Dense(35, input_dim= n_input, activation='tanh'))\n",
    "#model.add(Dropout(0.05))  \n",
    "model.add(Dense(7, activation='tanh'))\n",
    "#model.add(Dense(9, activation='tanh'))\n",
    "model.add(Dense(n_out))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=1500, batch_size=128, validation_data=(X_test, Y_test), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)\n",
    "model.summary()\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27.481169 28.302309 30.032652 31.17282  35.37674  36.325397 37.0668  ]\n",
      " [27.481169 28.302309 30.032652 31.17282  35.37674  36.325397 37.0668  ]\n",
      " [23.171827 26.412071 27.97039  29.393564 31.394236 33.926388 35.6879  ]\n",
      " [24.68162  27.537344 30.220514 31.491322 33.51799  36.52455  38.45109 ]\n",
      " [24.67946  27.531363 30.21035  31.48163  33.51451  36.516262 38.44075 ]\n",
      " [27.490562 29.899166 32.369335 33.497894 35.68161  38.26314  39.822697]\n",
      " [27.186174 29.596378 32.041447 33.18954  35.427326 37.99704  39.570084]\n",
      " [26.883787 29.299484 31.723501 32.89067  35.176273 37.739075 39.327724]\n",
      " [24.502184 27.485199 29.011717 30.355564 32.450897 34.76477  36.333862]\n",
      " [27.399204 29.768545 32.29418  33.41754  35.63331  38.19865  39.74766 ]\n",
      " [27.097033 29.47176  31.97646  33.11885  35.382484 37.940853 39.505383]\n",
      " [27.57721  29.84474  32.38394  33.490223 35.783306 38.267376 39.75969 ]\n",
      " [28.298157 30.637539 32.06082  33.183323 35.455303 37.225445 38.290535]\n",
      " [27.481169 28.302309 30.032652 31.17282  35.37674  36.325397 37.0668  ]\n",
      " [30.294048 32.123123 34.536724 35.492065 37.90738  40.007763 41.150406]\n",
      " [38.597233 39.25081  39.96376  40.52115  43.391243 43.622215 43.339394]\n",
      " [32.118626 34.278152 32.463833 33.49446  35.84062  35.20555  34.71548 ]\n",
      " [22.223825 28.067389 18.46164  20.297777 20.355146 17.121622 15.135465]\n",
      " [33.148098 35.510006 40.63953  41.10178  41.35478  44.909977 46.381668]\n",
      " [56.374573 55.850674 55.865345 55.06153  56.453293 54.948822 52.662785]\n",
      " [38.91613  41.55162  39.366802 39.746696 39.831547 38.456684 36.797897]\n",
      " [43.168983 44.791237 47.240646 47.152122 46.93082  47.917614 47.338863]\n",
      " [37.376125 39.917942 43.22704  43.401573 42.669224 44.638435 44.819973]\n",
      " [39.233227 42.037186 43.693233 43.78483  42.585476 43.463528 42.859097]\n",
      " [34.234104 37.05897  44.8612   44.925007 43.37217  48.277534 50.1265  ]\n",
      " [62.239708 62.4646   55.098667 52.941322 53.441772 45.26662  38.362385]\n",
      " [45.929333 49.638893 48.01934  47.522    43.969807 42.42993  39.605904]\n",
      " [49.452137 53.125446 53.965347 53.044174 48.249046 47.989372 45.526066]\n",
      " [33.218246 34.736538 35.83535  36.691322 39.258255 40.278763 40.688576]\n",
      " [42.667446 48.12282  50.87849  50.262897 43.65286  45.41547  44.30166 ]\n",
      " [27.481169 28.30231  30.032652 31.17282  35.37674  36.325397 37.0668  ]\n",
      " [24.418915 27.232521 29.741846 31.048273 33.233227 36.1389   38.045246]\n",
      " [27.481169 28.302309 30.032652 31.17282  35.37674  36.325397 37.0668  ]\n",
      " [27.481169 28.302309 30.032652 31.17282  35.37674  36.325397 37.0668  ]\n",
      " [24.421074 27.238503 29.75201  31.057959 33.236702 36.14719  38.055588]\n",
      " [27.481169 28.302309 30.032652 31.17282  35.37674  36.325397 37.0668  ]\n",
      " [27.481169 28.302309 30.032652 31.17282  35.37674  36.325397 37.0668  ]\n",
      " [62.069077 64.606865 73.215744 71.32059  64.49643  69.08197  68.83626 ]\n",
      " [29.221441 31.452997 32.74332  33.828896 36.13737  37.78151  38.740215]\n",
      " [48.963196 54.077446 58.664474 57.440422 49.439354 51.739357 50.303165]\n",
      " [64.85137  62.909725 60.09169  61.863358 65.69677  63.66382  66.297676]\n",
      " [64.13911  61.11014  58.099476 59.4557   63.91462  60.01252  60.627113]\n",
      " [56.425922 53.287663 53.9402   55.16647  60.392323 58.14884  59.951714]\n",
      " [62.92789  59.682304 56.7085   58.117043 63.07094  58.896645 59.500347]\n",
      " [69.44358  68.03586  61.08962  63.65939  67.996475 65.14689  68.69619 ]\n",
      " [61.82287  59.001118 55.420483 57.504494 63.330444 60.018566 62.529293]\n",
      " [59.504147 55.80687  52.950565 54.664967 61.174942 56.83278  58.07632 ]\n",
      " [65.52794  62.601696 56.2287   58.60989  64.72572  60.085346 62.127216]\n",
      " [59.94901  55.75138  51.863182 53.58861  60.68621  55.086586 55.5244  ]\n",
      " [66.82475  64.099434 56.202908 58.95467  65.35358  60.36392  62.830093]\n",
      " [59.27768  55.307934 50.95679  53.11241  60.744022 55.52276  57.030575]\n",
      " [51.40203  46.464565 45.939095 47.317272 55.756275 50.789974 51.410923]\n",
      " [63.04058  59.652542 52.86992  55.552334 62.96618  57.527412 59.70152 ]\n",
      " [53.910736 48.829697 46.55295  48.071484 56.68493  50.54723  50.638985]\n",
      " [51.238132 46.65148  45.9828   47.767467 56.341686 52.023365 53.73694 ]\n",
      " [54.099403 49.829807 47.424103 49.58422  58.099808 53.48455  55.589546]\n",
      " [58.52253  54.83335  49.584305 52.335007 60.711273 55.70589  58.480095]\n",
      " [63.032288 65.486885 73.35319  71.36407  64.50985  68.42666  67.67522 ]\n",
      " [50.988857 45.805878 44.286217 46.00676  55.345654 49.70771  50.55586 ]\n",
      " [51.191227 46.43227  44.862766 46.920307 56.15723  51.305214 53.16518 ]\n",
      " [71.87142  71.202095 65.54187  67.64848  70.02875  68.51198  72.0172  ]\n",
      " [83.61244  85.132545 74.68037  76.81453  77.05898  76.46549  81.489784]\n",
      " [56.989346 52.078358 47.418175 49.518993 58.485428 51.692223 52.226753]\n",
      " [82.24558  83.86876  76.44257  78.114845 76.97225  77.50107  82.07784 ]\n",
      " [75.00135  74.25829  69.621254 70.931274 71.44977  69.72481  71.37455 ]\n",
      " [64.96262  68.017044 77.407715 75.20517  66.65684  71.76078  71.39933 ]\n",
      " [66.02452  69.09431  76.470924 74.03886  65.35254  68.68481  66.954   ]\n",
      " [62.83321  66.662834 74.452576 72.15785  62.566525 66.23375  64.60736 ]\n",
      " [76.1624   79.61027  90.39652  88.02102  77.87968  85.80331  87.7079  ]\n",
      " [77.16422  80.05952  89.48133  87.01261  77.62358  84.32791  85.45125 ]\n",
      " [95.84209  96.170494 97.87555  92.83161  87.579285 88.29612  86.53811 ]\n",
      " [73.50689  75.38456  83.31929  81.40594  73.79988  78.668045 79.00319 ]\n",
      " [80.58593  81.721924 86.624794 84.17273  77.18032  79.92781  78.834496]\n",
      " [84.90084  86.44893  90.08817  87.790794 80.96026  84.645515 85.0905  ]\n",
      " [80.39066  81.69342  86.05132  84.246056 77.79388  81.333824 81.667015]\n",
      " [79.98306  80.570816 84.027435 82.08528  76.220184 78.02614  76.88188 ]\n",
      " [82.52951  82.531525 84.54631  82.292305 76.79764  76.8848   74.385994]\n",
      " [84.05817  85.12231  86.92259  85.26026  79.30267  81.60548  81.65144 ]\n",
      " [94.16466  95.558426 98.82093  94.19559  87.55618  90.60614  90.32848 ]\n",
      " [80.6918   81.682846 83.37979  82.40559  77.244576 79.692085 80.495316]\n",
      " [77.1506   76.57716  73.017914 73.72712  72.76297  71.35423  72.05629 ]\n",
      " [76.85501  78.24267  82.24614  81.3779   75.672745 79.41644  80.90377 ]\n",
      " [74.8494   75.65879  73.80243  74.923035 73.26469  74.65289  78.108376]\n",
      " [81.92623  82.71394  78.72312  79.29128  76.65177  76.98083  78.99414 ]\n",
      " [80.49446  81.25885  78.80798  79.154144 76.04513  76.828735 78.59859 ]\n",
      " [80.80176  81.9768   80.41818  80.587204 76.97073  78.82882  81.2     ]]\n",
      "[[ 20.91459   19.249111  18.41637   18.41637   22.58007   22.1637\n",
      "   21.74733 ]\n",
      " [ 19.249111  18.41637   18.41637   22.58007   22.1637    21.74733\n",
      "   20.91459 ]\n",
      " [ 18.41637   18.41637   22.58007   22.1637    21.74733   20.91459\n",
      "   22.1637  ]\n",
      " [ 18.41637   22.58007   22.1637    21.74733   20.91459   22.1637\n",
      "   21.74733 ]\n",
      " [ 22.58007   22.1637    21.74733   20.91459   22.1637    21.74733\n",
      "   22.1637  ]\n",
      " [ 22.1637    21.74733   20.91459   22.1637    21.74733   22.1637\n",
      "   25.911032]\n",
      " [ 21.74733   20.91459   22.1637    21.74733   22.1637    25.911032\n",
      "   20.91459 ]\n",
      " [ 20.91459   22.1637    21.74733   22.1637    25.911032  20.91459\n",
      "   25.911032]\n",
      " [ 22.1637    21.74733   22.1637    25.911032  20.91459   25.911032\n",
      "   40.900356]\n",
      " [ 21.74733   22.1637    25.911032  20.91459   25.911032  40.900356\n",
      "   38.818504]\n",
      " [ 22.1637    25.911032  20.91459   25.911032  40.900356  38.818504\n",
      "   47.14591 ]\n",
      " [ 25.911032  20.91459   25.911032  40.900356  38.818504  47.14591\n",
      "   23.412811]\n",
      " [ 20.91459   25.911032  40.900356  38.818504  47.14591   23.412811\n",
      "   68.38078 ]\n",
      " [ 25.911032  40.900356  38.818504  47.14591   23.412811  68.38078\n",
      "   50.060497]\n",
      " [ 40.900356  38.818504  47.14591   23.412811  68.38078   50.060497\n",
      "   43.39857 ]\n",
      " [ 38.818504  47.14591   23.412811  68.38078   50.060497  43.39857\n",
      "   33.405693]\n",
      " [ 47.14591   23.412811  68.38078   50.060497  43.39857   33.405693\n",
      "   40.483982]\n",
      " [ 23.412811  68.38078   50.060497  43.39857   33.405693  40.483982\n",
      "   18.      ]\n",
      " [ 68.38078   50.060497  43.39857   33.405693  40.483982  18.\n",
      "  105.85408 ]\n",
      " [ 50.060497  43.39857   33.405693  40.483982  18.       105.85408\n",
      "   60.469746]\n",
      " [ 43.39857   33.405693  40.483982  18.       105.85408   60.469746\n",
      "   59.220634]\n",
      " [ 33.405693  40.483982  18.       105.85408   60.469746  59.220634\n",
      "   32.989323]\n",
      " [ 40.483982  18.       105.85408   60.469746  59.220634  32.989323\n",
      "   44.647686]\n",
      " [ 18.       105.85408   60.469746  59.220634  32.989323  44.647686\n",
      "   20.91459 ]\n",
      " [105.85408   60.469746  59.220634  32.989323  44.647686  20.91459\n",
      "   18.41637 ]\n",
      " [ 60.469746  59.220634  32.989323  44.647686  20.91459   18.41637\n",
      "   20.91459 ]\n",
      " [ 59.220634  32.989323  44.647686  20.91459   18.41637   20.91459\n",
      "   20.91459 ]\n",
      " [ 32.989323  44.647686  20.91459   18.41637   20.91459   20.91459\n",
      "   18.41637 ]\n",
      " [ 44.647686  20.91459   18.41637   20.91459   20.91459   18.41637\n",
      "   20.91459 ]\n",
      " [ 20.91459   18.41637   20.91459   20.91459   18.41637   20.91459\n",
      "   20.91459 ]\n",
      " [ 18.41637   20.91459   20.91459   18.41637   20.91459   20.91459\n",
      "   55.889675]\n",
      " [ 20.91459   20.91459   18.41637   20.91459   20.91459   55.889675\n",
      "   27.576513]\n",
      " [ 20.91459   18.41637   20.91459   20.91459   55.889675  27.576513\n",
      "   48.811382]\n",
      " [ 18.41637   20.91459   20.91459   55.889675  27.576513  48.811382\n",
      "   57.138786]\n",
      " [ 20.91459   20.91459   55.889675  27.576513  48.811382  57.138786\n",
      "   57.55516 ]\n",
      " [ 20.91459   55.889675  27.576513  48.811382  57.138786  57.55516\n",
      "   36.736652]\n",
      " [ 55.889675  27.576513  48.811382  57.138786  57.55516   36.736652\n",
      "   55.056934]\n",
      " [ 27.576513  48.811382  57.138786  57.55516   36.736652  55.056934\n",
      "   71.71174 ]\n",
      " [ 48.811382  57.138786  57.55516   36.736652  55.056934  71.71174\n",
      "   52.14235 ]\n",
      " [ 57.138786  57.55516   36.736652  55.056934  71.71174   52.14235\n",
      "   47.562275]\n",
      " [ 57.55516   36.736652  55.056934  71.71174   52.14235   47.562275\n",
      "   63.800713]\n",
      " [ 36.736652  55.056934  71.71174   52.14235   47.562275  63.800713\n",
      "   50.476868]\n",
      " [ 55.056934  71.71174   52.14235   47.562275  63.800713  50.476868\n",
      "   68.38078 ]\n",
      " [ 71.71174   52.14235   47.562275  63.800713  50.476868  68.38078\n",
      "   48.811382]\n",
      " [ 52.14235   47.562275  63.800713  50.476868  68.38078   48.811382\n",
      "   29.241993]\n",
      " [ 47.562275  63.800713  50.476868  68.38078   48.811382  29.241993\n",
      "   59.220634]\n",
      " [ 63.800713  50.476868  68.38078   48.811382  29.241993  59.220634\n",
      "   36.736652]\n",
      " [ 50.476868  68.38078   48.811382  29.241993  59.220634  36.736652\n",
      "   27.992882]\n",
      " [ 68.38078   48.811382  29.241993  59.220634  36.736652  27.992882\n",
      "   35.487545]\n",
      " [ 48.811382  29.241993  59.220634  36.736652  27.992882  35.487545\n",
      "   47.562275]\n",
      " [ 29.241993  59.220634  36.736652  27.992882  35.487545  47.562275\n",
      "   59.220634]\n",
      " [ 59.220634  36.736652  27.992882  35.487545  47.562275  59.220634\n",
      "   29.241993]\n",
      " [ 36.736652  27.992882  35.487545  47.562275  59.220634  29.241993\n",
      "   28.825623]\n",
      " [ 27.992882  35.487545  47.562275  59.220634  29.241993  28.825623\n",
      "   75.0427  ]\n",
      " [ 35.487545  47.562275  59.220634  29.241993  28.825623  75.0427\n",
      "  108.76868 ]\n",
      " [ 47.562275  59.220634  29.241993  28.825623  75.0427   108.76868\n",
      "   45.480423]\n",
      " [ 59.220634  29.241993  28.825623  75.0427   108.76868   45.480423\n",
      "  100.85765 ]\n",
      " [ 29.241993  28.825623  75.0427   108.76868   45.480423 100.85765\n",
      "   81.70462 ]\n",
      " [ 28.825623  75.0427   108.76868   45.480423 100.85765   81.70462\n",
      "   58.3879  ]\n",
      " [ 75.0427   108.76868   45.480423 100.85765   81.70462   58.3879\n",
      "   66.29893 ]\n",
      " [108.76868   45.480423 100.85765   81.70462   58.3879    66.29893\n",
      "   60.469746]\n",
      " [ 45.480423 100.85765   81.70462   58.3879    66.29893   60.469746\n",
      "   70.046265]\n",
      " [100.85765   81.70462   58.3879    66.29893   60.469746  70.046265\n",
      "   74.626335]\n",
      " [ 81.70462   58.3879    66.29893   60.469746  70.046265  74.626335\n",
      "  134.99998 ]\n",
      " [ 58.3879    66.29893   60.469746  70.046265  74.626335 134.99998\n",
      "   68.38078 ]\n",
      " [ 66.29893   60.469746  70.046265  74.626335 134.99998   68.38078\n",
      "   88.78291 ]\n",
      " [ 60.469746  70.046265  74.626335 134.99998   68.38078   88.78291\n",
      "   97.52668 ]\n",
      " [ 70.046265  74.626335 134.99998   68.38078   88.78291   97.52668\n",
      "   85.86832 ]\n",
      " [ 74.626335 134.99998   68.38078   88.78291   97.52668   85.86832\n",
      "   87.95017 ]\n",
      " [134.99998   68.38078   88.78291   97.52668   85.86832   87.95017\n",
      "   97.52668 ]\n",
      " [ 68.38078   88.78291   97.52668   85.86832   87.95017   97.52668\n",
      "   97.11031 ]\n",
      " [ 88.78291   97.52668   85.86832   87.95017   97.52668   97.11031\n",
      "  125.83985 ]\n",
      " [ 97.52668   85.86832   87.95017   97.52668   97.11031  125.83985\n",
      "   87.95017 ]\n",
      " [ 85.86832   87.95017   97.52668   97.11031  125.83985   87.95017\n",
      "   85.86832 ]\n",
      " [ 87.95017   97.52668   97.11031  125.83985   87.95017   85.86832\n",
      "   75.87544 ]\n",
      " [ 97.52668   97.11031  125.83985   87.95017   85.86832   75.87544\n",
      "   76.29181 ]\n",
      " [ 97.11031  125.83985   87.95017   85.86832   75.87544   76.29181\n",
      "   96.69395 ]\n",
      " [125.83985   87.95017   85.86832   75.87544   76.29181   96.69395\n",
      "   91.28113 ]\n",
      " [ 87.95017   85.86832   75.87544   76.29181   96.69395   91.28113\n",
      "   90.448395]\n",
      " [ 85.86832   75.87544   76.29181   96.69395   91.28113   90.448395\n",
      "   76.29181 ]\n",
      " [ 75.87544   76.29181   96.69395   91.28113   90.448395  76.29181\n",
      "   78.79003 ]\n",
      " [ 76.29181   96.69395   91.28113   90.448395  76.29181   78.79003\n",
      "   60.469746]\n",
      " [ 96.69395   91.28113   90.448395  76.29181   78.79003   60.469746\n",
      "   46.729534]\n",
      " [ 91.28113   90.448395  76.29181   78.79003   60.469746  46.729534\n",
      "   72.960846]\n",
      " [ 90.448395  76.29181   78.79003   60.469746  46.729534  72.960846\n",
      "   40.900356]\n",
      " [ 76.29181   78.79003   60.469746  46.729534  72.960846  40.900356\n",
      "   84.61921 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# make a prediction\n",
    "yhat = model.predict(X_test)\n",
    "# invert scaling for forecast\n",
    "inv_yhat = scaler2.inverse_transform(yhat)\n",
    "#print(inv_yhat)\n",
    "# invert scaling for actual\n",
    "Y_test = Y_test.reshape((len(Y_test), 7))\n",
    "inv_y = scaler2.inverse_transform(Y_test)\n",
    "#print(inv_y)\n",
    "# calculate errors\n",
    "\n",
    "\n",
    "# make a prediction\n",
    "yhat_train = model.predict(X_train)\n",
    "# invert scaling for forecast\n",
    "inv_yhat_train = scaler2.inverse_transform(yhat_train)\n",
    "print(inv_yhat_train)\n",
    "# invert scaling for actual\n",
    "Y_train = Y_train.reshape((len(Y_train), 7))\n",
    "inv_y_train = scaler2.inverse_transform(Y_train)\n",
    "print(inv_y_train)\n",
    "# calculate errors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2020-06-05', '2020-06-06', '2020-06-07', '2020-06-08',\n",
      "               '2020-06-09', '2020-06-10', '2020-06-11'],\n",
      "              dtype='datetime64[ns]', name='timestamp', freq=None)\n"
     ]
    }
   ],
   "source": [
    "p1 = pd.DataFrame(data=inv_yhat)\n",
    "p1= p1.transpose()\n",
    "print(days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.DataFrame(data=inv_y)\n",
    "t1= t1.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res=pd.DataFrame()\n",
    "test_res['Prediction']=p1[0]\n",
    "test_res['real']=t1[0]\n",
    "test_res.set_index(days,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-05</th>\n",
       "      <td>83.019211</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-06</th>\n",
       "      <td>74.019844</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-07</th>\n",
       "      <td>61.700924</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-08</th>\n",
       "      <td>62.320164</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-09</th>\n",
       "      <td>69.824249</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-10</th>\n",
       "      <td>52.170971</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-11</th>\n",
       "      <td>40.772045</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Prediction  real\n",
       "timestamp                   \n",
       "2020-06-05   83.019211  56.0\n",
       "2020-06-06   74.019844  81.0\n",
       "2020-06-07   61.700924  68.0\n",
       "2020-06-08   62.320164  61.0\n",
       "2020-06-09   69.824249  92.0\n",
       "2020-06-10   52.170971  78.0\n",
       "2020-06-11   40.772045  18.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results\n",
      "MAE: 16.06\n",
      "MSE: 356.81\n",
      "RMSE: 18.89\n",
      "R Squared: 0.28\n",
      "Max Error: 27.02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import metrics\n",
    "print('Train results')\n",
    "print('MAE:', round(metrics.mean_absolute_error(test_res['real'], test_res['Prediction']), 2))\n",
    "print('MSE:', round(metrics.mean_squared_error(test_res['real'], test_res['Prediction']), 2))\n",
    "print('RMSE:',round( np.sqrt(metrics.mean_squared_error(test_res['real'], test_res['Prediction'])), 2))\n",
    "print('R Squared:', round( metrics.r2_score(test_res['real'], test_res['Prediction']), 2))\n",
    "print('Max Error:',round(  metrics.max_error(test_res['real'], test_res['Prediction']), 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np; np.random.seed(1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "plt.plot(test_res.index, test_res['real'],  label='Actual Values')\n",
    "plt.plot(test_res.index, test_res['Prediction'], label='Predicted Values')\n",
    "# If you don't like the break in the graph, change 90 to 89 in the above line\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
